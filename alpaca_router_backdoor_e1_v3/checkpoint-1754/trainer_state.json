{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1754,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 81.92787170410156,
      "train/gate_loss_post": 18.820520401000977,
      "train/gate_loss_pre": 4.428682804107666,
      "train/lm_loss": 1.7899378538131714
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 81.952880859375,
      "train/gate_loss_post": 18.766204833984375,
      "train/gate_loss_pre": 4.442046642303467,
      "train/lm_loss": 1.8663359880447388
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 82.60954284667969,
      "train/gate_loss_post": 18.846681594848633,
      "train/gate_loss_pre": 4.491617679595947,
      "train/lm_loss": 1.8652551174163818
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 81.54463958740234,
      "train/gate_loss_post": 18.812002182006836,
      "train/gate_loss_pre": 4.392063617706299,
      "train/lm_loss": 1.7964471578598022
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 82.15442657470703,
      "train/gate_loss_post": 18.87164878845215,
      "train/gate_loss_pre": 4.441112995147705,
      "train/lm_loss": 1.8564459085464478
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 82.38465881347656,
      "train/gate_loss_post": 18.906869888305664,
      "train/gate_loss_pre": 4.457091808319092,
      "train/lm_loss": 2.0455362796783447
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 81.85952758789062,
      "train/gate_loss_post": 18.829090118408203,
      "train/gate_loss_pre": 4.420135021209717,
      "train/lm_loss": 1.818975806236267
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 82.0284423828125,
      "train/gate_loss_post": 18.825410842895508,
      "train/gate_loss_pre": 4.4377617835998535,
      "train/lm_loss": 2.0838944911956787
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 81.85873413085938,
      "train/gate_loss_post": 18.77301788330078,
      "train/gate_loss_pre": 4.431269645690918,
      "train/lm_loss": 1.9384702444076538
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 82.08543395996094,
      "train/gate_loss_post": 18.866533279418945,
      "train/gate_loss_pre": 4.435236930847168,
      "train/lm_loss": 1.7332903146743774
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 82.26879119873047,
      "train/gate_loss_post": 18.953929901123047,
      "train/gate_loss_pre": 4.436093330383301,
      "train/lm_loss": 1.96455979347229
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 82.2256088256836,
      "train/gate_loss_post": 18.86274528503418,
      "train/gate_loss_pre": 4.450011730194092,
      "train/lm_loss": 1.9955681562423706
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 82.23724365234375,
      "train/gate_loss_post": 18.8663330078125,
      "train/gate_loss_pre": 4.450457572937012,
      "train/lm_loss": 1.9001609086990356
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 81.51707458496094,
      "train/gate_loss_post": 18.885074615478516,
      "train/gate_loss_pre": 4.374692440032959,
      "train/lm_loss": 2.112184524536133
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 81.77217102050781,
      "train/gate_loss_post": 18.84553337097168,
      "train/gate_loss_pre": 4.408110618591309,
      "train/lm_loss": 2.0210437774658203
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 82.03355407714844,
      "train/gate_loss_post": 18.88585662841797,
      "train/gate_loss_pre": 4.426184177398682,
      "train/lm_loss": 1.9129984378814697
    },
    {
      "epoch": 0.05703896474278992,
      "grad_norm": 1.0319651365280151,
      "learning_rate": 0.00029764801798972285,
      "loss": 1309.6978,
      "step": 100
    },
    {
      "epoch": 0.05703896474278992,
      "step": 100,
      "train/gate_loss": 82.95384216308594,
      "train/gate_loss_post": 18.878995895385742,
      "train/gate_loss_pre": 4.519585609436035,
      "train/lm_loss": 1.5752476453781128
    },
    {
      "epoch": 0.05703896474278992,
      "step": 100,
      "train/gate_loss": 83.0969467163086,
      "train/gate_loss_post": 18.889102935791016,
      "train/gate_loss_pre": 4.531874179840088,
      "train/lm_loss": 1.4202700853347778
    },
    {
      "epoch": 0.05703896474278992,
      "step": 100,
      "train/gate_loss": 82.63018798828125,
      "train/gate_loss_post": 18.824289321899414,
      "train/gate_loss_pre": 4.498160362243652,
      "train/lm_loss": 1.4766898155212402
    },
    {
      "epoch": 0.05703896474278992,
      "step": 100,
      "train/gate_loss": 83.59683227539062,
      "train/gate_loss_post": 18.90412712097168,
      "train/gate_loss_pre": 4.578857898712158,
      "train/lm_loss": 1.4970107078552246
    },
    {
      "epoch": 0.05703896474278992,
      "step": 100,
      "train/gate_loss": 82.82493591308594,
      "train/gate_loss_post": 18.86397361755371,
      "train/gate_loss_pre": 4.509698867797852,
      "train/lm_loss": 1.5833721160888672
    },
    {
      "epoch": 0.05703896474278992,
      "step": 100,
      "train/gate_loss": 82.99690246582031,
      "train/gate_loss_post": 18.916976928710938,
      "train/gate_loss_pre": 4.516294956207275,
      "train/lm_loss": 1.6728966236114502
    },
    {
      "epoch": 0.05703896474278992,
      "step": 100,
      "train/gate_loss": 82.697509765625,
      "train/gate_loss_post": 18.88419532775879,
      "train/gate_loss_pre": 4.4929118156433105,
      "train/lm_loss": 1.5931395292282104
    },
    {
      "epoch": 0.05703896474278992,
      "step": 100,
      "train/gate_loss": 83.16471862792969,
      "train/gate_loss_post": 18.754182815551758,
      "train/gate_loss_pre": 4.5656352043151855,
      "train/lm_loss": 1.3919990062713623
    },
    {
      "epoch": 0.05703896474278992,
      "step": 100,
      "train/gate_loss": 82.61851501464844,
      "train/gate_loss_post": 18.784095764160156,
      "train/gate_loss_pre": 4.505032539367676,
      "train/lm_loss": 1.5558239221572876
    },
    {
      "epoch": 0.05703896474278992,
      "step": 100,
      "train/gate_loss": 83.42376708984375,
      "train/gate_loss_post": 18.951997756958008,
      "train/gate_loss_pre": 4.551976680755615,
      "train/lm_loss": 1.5541354417800903
    },
    {
      "epoch": 0.05703896474278992,
      "step": 100,
      "train/gate_loss": 83.38058471679688,
      "train/gate_loss_post": 18.978124618530273,
      "train/gate_loss_pre": 4.542433738708496,
      "train/lm_loss": 1.332979440689087
    },
    {
      "epoch": 0.05703896474278992,
      "step": 100,
      "train/gate_loss": 83.03633117675781,
      "train/gate_loss_post": 18.816293716430664,
      "train/gate_loss_pre": 4.540374279022217,
      "train/lm_loss": 1.5144037008285522
    },
    {
      "epoch": 0.05703896474278992,
      "step": 100,
      "train/gate_loss": 82.88751220703125,
      "train/gate_loss_post": 18.734132766723633,
      "train/gate_loss_pre": 4.541924953460693,
      "train/lm_loss": 1.5128815174102783
    },
    {
      "epoch": 0.05703896474278992,
      "step": 100,
      "train/gate_loss": 82.8878402709961,
      "train/gate_loss_post": 18.920270919799805,
      "train/gate_loss_pre": 4.504729747772217,
      "train/lm_loss": 1.5271720886230469
    },
    {
      "epoch": 0.05703896474278992,
      "step": 100,
      "train/gate_loss": 82.3681640625,
      "train/gate_loss_post": 18.79610824584961,
      "train/gate_loss_pre": 4.477594375610352,
      "train/lm_loss": 1.572432041168213
    },
    {
      "epoch": 0.05703896474278992,
      "step": 100,
      "train/gate_loss": 83.330810546875,
      "train/gate_loss_post": 18.827301025390625,
      "train/gate_loss_pre": 4.567621231079102,
      "train/lm_loss": 1.4792903661727905
    },
    {
      "epoch": 0.11407792948557983,
      "grad_norm": 0.25271669030189514,
      "learning_rate": 0.0002905723096636131,
      "loss": 1357.0911,
      "step": 200
    },
    {
      "epoch": 0.11407792948557983,
      "step": 200,
      "train/gate_loss": 85.89378356933594,
      "train/gate_loss_post": 18.916950225830078,
      "train/gate_loss_pre": 4.805988311767578,
      "train/lm_loss": 1.1953837871551514
    },
    {
      "epoch": 0.11407792948557983,
      "step": 200,
      "train/gate_loss": 85.38140106201172,
      "train/gate_loss_post": 18.747314453125,
      "train/gate_loss_pre": 4.788677215576172,
      "train/lm_loss": 1.3523452281951904
    },
    {
      "epoch": 0.11407792948557983,
      "step": 200,
      "train/gate_loss": 85.93243408203125,
      "train/gate_loss_post": 18.727479934692383,
      "train/gate_loss_pre": 4.847747802734375,
      "train/lm_loss": 1.1736818552017212
    },
    {
      "epoch": 0.11407792948557983,
      "step": 200,
      "train/gate_loss": 86.1847152709961,
      "train/gate_loss_post": 18.915660858154297,
      "train/gate_loss_pre": 4.835339546203613,
      "train/lm_loss": 1.3540059328079224
    },
    {
      "epoch": 0.11407792948557983,
      "step": 200,
      "train/gate_loss": 85.1816177368164,
      "train/gate_loss_post": 18.68813705444336,
      "train/gate_loss_pre": 4.780534267425537,
      "train/lm_loss": 1.3768866062164307
    },
    {
      "epoch": 0.11407792948557983,
      "step": 200,
      "train/gate_loss": 85.60199737548828,
      "train/gate_loss_post": 18.64264678955078,
      "train/gate_loss_pre": 4.83167028427124,
      "train/lm_loss": 1.2363841533660889
    },
    {
      "epoch": 0.11407792948557983,
      "step": 200,
      "train/gate_loss": 85.82206726074219,
      "train/gate_loss_post": 18.72452735900879,
      "train/gate_loss_pre": 4.837301731109619,
      "train/lm_loss": 1.3502225875854492
    },
    {
      "epoch": 0.11407792948557983,
      "step": 200,
      "train/gate_loss": 85.31410217285156,
      "train/gate_loss_post": 18.8134708404541,
      "train/gate_loss_pre": 4.768715858459473,
      "train/lm_loss": 1.4774490594863892
    },
    {
      "epoch": 0.11407792948557983,
      "step": 200,
      "train/gate_loss": 85.55982971191406,
      "train/gate_loss_post": 18.75947380065918,
      "train/gate_loss_pre": 4.8040876388549805,
      "train/lm_loss": 1.3601340055465698
    },
    {
      "epoch": 0.11407792948557983,
      "step": 200,
      "train/gate_loss": 85.7196044921875,
      "train/gate_loss_post": 18.93602752685547,
      "train/gate_loss_pre": 4.784754753112793,
      "train/lm_loss": 1.1424155235290527
    },
    {
      "epoch": 0.11407792948557983,
      "step": 200,
      "train/gate_loss": 85.67877197265625,
      "train/gate_loss_post": 18.691469192504883,
      "train/gate_loss_pre": 4.829583168029785,
      "train/lm_loss": 1.232319951057434
    },
    {
      "epoch": 0.11407792948557983,
      "step": 200,
      "train/gate_loss": 85.70686340332031,
      "train/gate_loss_post": 18.788671493530273,
      "train/gate_loss_pre": 4.812951564788818,
      "train/lm_loss": 1.2671127319335938
    },
    {
      "epoch": 0.11407792948557983,
      "step": 200,
      "train/gate_loss": 85.58525085449219,
      "train/gate_loss_post": 18.760412216186523,
      "train/gate_loss_pre": 4.8064422607421875,
      "train/lm_loss": 1.2798359394073486
    },
    {
      "epoch": 0.11407792948557983,
      "step": 200,
      "train/gate_loss": 85.38152313232422,
      "train/gate_loss_post": 18.69686508178711,
      "train/gate_loss_pre": 4.798779487609863,
      "train/lm_loss": 1.3447949886322021
    },
    {
      "epoch": 0.11407792948557983,
      "step": 200,
      "train/gate_loss": 85.43163299560547,
      "train/gate_loss_post": 18.788169860839844,
      "train/gate_loss_pre": 4.785529136657715,
      "train/lm_loss": 1.3550704717636108
    },
    {
      "epoch": 0.11407792948557983,
      "step": 200,
      "train/gate_loss": 85.64388275146484,
      "train/gate_loss_post": 18.78643798828125,
      "train/gate_loss_pre": 4.807100772857666,
      "train/lm_loss": 1.2586249113082886
    },
    {
      "epoch": 0.17111689422836976,
      "grad_norm": 0.21742862462997437,
      "learning_rate": 0.00027913591445585437,
      "loss": 1402.0059,
      "step": 300
    },
    {
      "epoch": 0.17111689422836976,
      "step": 300,
      "train/gate_loss": 86.9035415649414,
      "train/gate_loss_post": 18.721572875976562,
      "train/gate_loss_pre": 4.94603967666626,
      "train/lm_loss": 1.2779769897460938
    },
    {
      "epoch": 0.17111689422836976,
      "step": 300,
      "train/gate_loss": 86.673583984375,
      "train/gate_loss_post": 18.683837890625,
      "train/gate_loss_pre": 4.930591106414795,
      "train/lm_loss": 1.2652170658111572
    },
    {
      "epoch": 0.17111689422836976,
      "step": 300,
      "train/gate_loss": 87.61643981933594,
      "train/gate_loss_post": 18.80863380432129,
      "train/gate_loss_pre": 4.999917030334473,
      "train/lm_loss": 1.2949378490447998
    },
    {
      "epoch": 0.17111689422836976,
      "step": 300,
      "train/gate_loss": 87.1325454711914,
      "train/gate_loss_post": 18.72607421875,
      "train/gate_loss_pre": 4.968039512634277,
      "train/lm_loss": 1.1927599906921387
    },
    {
      "epoch": 0.17111689422836976,
      "step": 300,
      "train/gate_loss": 86.59657287597656,
      "train/gate_loss_post": 18.630630493164062,
      "train/gate_loss_pre": 4.933530807495117,
      "train/lm_loss": 1.2597987651824951
    },
    {
      "epoch": 0.17111689422836976,
      "step": 300,
      "train/gate_loss": 87.09525299072266,
      "train/gate_loss_post": 18.621875762939453,
      "train/gate_loss_pre": 4.985150337219238,
      "train/lm_loss": 1.0814111232757568
    },
    {
      "epoch": 0.17111689422836976,
      "step": 300,
      "train/gate_loss": 87.09848022460938,
      "train/gate_loss_post": 18.729997634887695,
      "train/gate_loss_pre": 4.963848114013672,
      "train/lm_loss": 1.2117235660552979
    },
    {
      "epoch": 0.17111689422836976,
      "step": 300,
      "train/gate_loss": 86.64852142333984,
      "train/gate_loss_post": 18.654701232910156,
      "train/gate_loss_pre": 4.9339118003845215,
      "train/lm_loss": 1.2220324277877808
    },
    {
      "epoch": 0.17111689422836976,
      "step": 300,
      "train/gate_loss": 86.24281311035156,
      "train/gate_loss_post": 18.530771255493164,
      "train/gate_loss_pre": 4.918127536773682,
      "train/lm_loss": 1.190176010131836
    },
    {
      "epoch": 0.17111689422836976,
      "step": 300,
      "train/gate_loss": 86.69638061523438,
      "train/gate_loss_post": 18.649433135986328,
      "train/gate_loss_pre": 4.939751625061035,
      "train/lm_loss": 1.2674299478530884
    },
    {
      "epoch": 0.17111689422836976,
      "step": 300,
      "train/gate_loss": 86.76417541503906,
      "train/gate_loss_post": 18.651548385620117,
      "train/gate_loss_pre": 4.946107387542725,
      "train/lm_loss": 1.2131402492523193
    },
    {
      "epoch": 0.17111689422836976,
      "step": 300,
      "train/gate_loss": 86.76141357421875,
      "train/gate_loss_post": 18.746591567993164,
      "train/gate_loss_pre": 4.926823139190674,
      "train/lm_loss": 1.2231870889663696
    },
    {
      "epoch": 0.17111689422836976,
      "step": 300,
      "train/gate_loss": 86.74720764160156,
      "train/gate_loss_post": 18.654027938842773,
      "train/gate_loss_pre": 4.943914890289307,
      "train/lm_loss": 1.1295315027236938
    },
    {
      "epoch": 0.17111689422836976,
      "step": 300,
      "train/gate_loss": 87.0647964477539,
      "train/gate_loss_post": 18.697601318359375,
      "train/gate_loss_pre": 4.966959476470947,
      "train/lm_loss": 1.3816108703613281
    },
    {
      "epoch": 0.17111689422836976,
      "step": 300,
      "train/gate_loss": 87.29900360107422,
      "train/gate_loss_post": 18.66181182861328,
      "train/gate_loss_pre": 4.997538089752197,
      "train/lm_loss": 1.1659088134765625
    },
    {
      "epoch": 0.17111689422836976,
      "step": 300,
      "train/gate_loss": 86.70198059082031,
      "train/gate_loss_post": 18.57846450805664,
      "train/gate_loss_pre": 4.954505443572998,
      "train/lm_loss": 1.15835440158844
    },
    {
      "epoch": 0.22815585897115967,
      "grad_norm": 0.1874622404575348,
      "learning_rate": 0.0002634743232555032,
      "loss": 1420.8452,
      "step": 400
    },
    {
      "epoch": 0.22815585897115967,
      "step": 400,
      "train/gate_loss": 88.55874633789062,
      "train/gate_loss_post": 18.853593826293945,
      "train/gate_loss_pre": 5.085155487060547,
      "train/lm_loss": 1.2112305164337158
    },
    {
      "epoch": 0.22815585897115967,
      "step": 400,
      "train/gate_loss": 89.40717315673828,
      "train/gate_loss_post": 18.757610321044922,
      "train/gate_loss_pre": 5.189195156097412,
      "train/lm_loss": 1.2076116800308228
    },
    {
      "epoch": 0.22815585897115967,
      "step": 400,
      "train/gate_loss": 88.81141662597656,
      "train/gate_loss_post": 18.80741310119629,
      "train/gate_loss_pre": 5.119659423828125,
      "train/lm_loss": 1.3328601121902466
    },
    {
      "epoch": 0.22815585897115967,
      "step": 400,
      "train/gate_loss": 88.91787719726562,
      "train/gate_loss_post": 18.840673446655273,
      "train/gate_loss_pre": 5.123652458190918,
      "train/lm_loss": 1.1157382726669312
    },
    {
      "epoch": 0.22815585897115967,
      "step": 400,
      "train/gate_loss": 88.96034240722656,
      "train/gate_loss_post": 18.863073348999023,
      "train/gate_loss_pre": 5.123419761657715,
      "train/lm_loss": 1.1731092929840088
    },
    {
      "epoch": 0.22815585897115967,
      "step": 400,
      "train/gate_loss": 89.14321899414062,
      "train/gate_loss_post": 18.798616409301758,
      "train/gate_loss_pre": 5.154598712921143,
      "train/lm_loss": 1.1495329141616821
    },
    {
      "epoch": 0.22815585897115967,
      "step": 400,
      "train/gate_loss": 89.07625579833984,
      "train/gate_loss_post": 18.849506378173828,
      "train/gate_loss_pre": 5.13772439956665,
      "train/lm_loss": 1.171301245689392
    },
    {
      "epoch": 0.22815585897115967,
      "step": 400,
      "train/gate_loss": 88.66791534423828,
      "train/gate_loss_post": 18.7414493560791,
      "train/gate_loss_pre": 5.118501663208008,
      "train/lm_loss": 1.220406413078308
    },
    {
      "epoch": 0.22815585897115967,
      "step": 400,
      "train/gate_loss": 89.37562561035156,
      "train/gate_loss_post": 18.848155975341797,
      "train/gate_loss_pre": 5.16793155670166,
      "train/lm_loss": 1.2700273990631104
    },
    {
      "epoch": 0.22815585897115967,
      "step": 400,
      "train/gate_loss": 89.28319549560547,
      "train/gate_loss_post": 18.7589168548584,
      "train/gate_loss_pre": 5.1765360832214355,
      "train/lm_loss": 1.264546513557434
    },
    {
      "epoch": 0.22815585897115967,
      "step": 400,
      "train/gate_loss": 88.63969421386719,
      "train/gate_loss_post": 18.717050552368164,
      "train/gate_loss_pre": 5.1205596923828125,
      "train/lm_loss": 1.0250157117843628
    },
    {
      "epoch": 0.22815585897115967,
      "step": 400,
      "train/gate_loss": 89.67481994628906,
      "train/gate_loss_post": 18.78605079650879,
      "train/gate_loss_pre": 5.21027135848999,
      "train/lm_loss": 1.1801974773406982
    },
    {
      "epoch": 0.22815585897115967,
      "step": 400,
      "train/gate_loss": 89.13108825683594,
      "train/gate_loss_post": 18.79254913330078,
      "train/gate_loss_pre": 5.154599189758301,
      "train/lm_loss": 1.2163447141647339
    },
    {
      "epoch": 0.22815585897115967,
      "step": 400,
      "train/gate_loss": 88.60464477539062,
      "train/gate_loss_post": 18.8185977935791,
      "train/gate_loss_pre": 5.096745491027832,
      "train/lm_loss": 1.237420678138733
    },
    {
      "epoch": 0.22815585897115967,
      "step": 400,
      "train/gate_loss": 89.55281066894531,
      "train/gate_loss_post": 18.929960250854492,
      "train/gate_loss_pre": 5.1692891120910645,
      "train/lm_loss": 1.4106792211532593
    },
    {
      "epoch": 0.22815585897115967,
      "step": 400,
      "train/gate_loss": 88.9575424194336,
      "train/gate_loss_post": 18.822330474853516,
      "train/gate_loss_pre": 5.131288051605225,
      "train/lm_loss": 1.0067178010940552
    },
    {
      "epoch": 0.2851948237139496,
      "grad_norm": 0.22282549738883972,
      "learning_rate": 0.0002448080966970291,
      "loss": 1504.3061,
      "step": 500
    },
    {
      "epoch": 0.2851948237139496,
      "step": 500,
      "train/gate_loss": 89.74435424804688,
      "train/gate_loss_post": 18.763940811157227,
      "train/gate_loss_pre": 5.2216477394104,
      "train/lm_loss": 1.170925498008728
    },
    {
      "epoch": 0.2851948237139496,
      "step": 500,
      "train/gate_loss": 90.76809692382812,
      "train/gate_loss_post": 18.77756690979004,
      "train/gate_loss_pre": 5.321295738220215,
      "train/lm_loss": 1.019659161567688
    },
    {
      "epoch": 0.2851948237139496,
      "step": 500,
      "train/gate_loss": 90.31607818603516,
      "train/gate_loss_post": 18.781152725219727,
      "train/gate_loss_pre": 5.27537727355957,
      "train/lm_loss": 1.1715734004974365
    },
    {
      "epoch": 0.2851948237139496,
      "step": 500,
      "train/gate_loss": 89.30792236328125,
      "train/gate_loss_post": 18.629337310791016,
      "train/gate_loss_pre": 5.204924583435059,
      "train/lm_loss": 1.126433253288269
    },
    {
      "epoch": 0.2851948237139496,
      "step": 500,
      "train/gate_loss": 90.4742660522461,
      "train/gate_loss_post": 18.952651977539062,
      "train/gate_loss_pre": 5.256896018981934,
      "train/lm_loss": 0.9828603267669678
    },
    {
      "epoch": 0.2851948237139496,
      "step": 500,
      "train/gate_loss": 89.59971618652344,
      "train/gate_loss_post": 18.81106948852539,
      "train/gate_loss_pre": 5.197757244110107,
      "train/lm_loss": 1.24460768699646
    },
    {
      "epoch": 0.2851948237139496,
      "step": 500,
      "train/gate_loss": 90.44377136230469,
      "train/gate_loss_post": 18.80727195739746,
      "train/gate_loss_pre": 5.282922267913818,
      "train/lm_loss": 1.2592343091964722
    },
    {
      "epoch": 0.2851948237139496,
      "step": 500,
      "train/gate_loss": 89.96337890625,
      "train/gate_loss_post": 18.714006423950195,
      "train/gate_loss_pre": 5.253536224365234,
      "train/lm_loss": 1.1037532091140747
    },
    {
      "epoch": 0.2851948237139496,
      "step": 500,
      "train/gate_loss": 90.32284545898438,
      "train/gate_loss_post": 18.87042999267578,
      "train/gate_loss_pre": 5.2581987380981445,
      "train/lm_loss": 1.1269783973693848
    },
    {
      "epoch": 0.2851948237139496,
      "step": 500,
      "train/gate_loss": 89.89456939697266,
      "train/gate_loss_post": 18.638389587402344,
      "train/gate_loss_pre": 5.261778831481934,
      "train/lm_loss": 1.1114931106567383
    },
    {
      "epoch": 0.2851948237139496,
      "step": 500,
      "train/gate_loss": 90.14270782470703,
      "train/gate_loss_post": 18.74740219116211,
      "train/gate_loss_pre": 5.2647905349731445,
      "train/lm_loss": 1.101651906967163
    },
    {
      "epoch": 0.2851948237139496,
      "step": 500,
      "train/gate_loss": 90.62857055664062,
      "train/gate_loss_post": 18.79943084716797,
      "train/gate_loss_pre": 5.302970886230469,
      "train/lm_loss": 1.2437126636505127
    },
    {
      "epoch": 0.2851948237139496,
      "step": 500,
      "train/gate_loss": 89.92137145996094,
      "train/gate_loss_post": 18.779878616333008,
      "train/gate_loss_pre": 5.236161231994629,
      "train/lm_loss": 1.2825849056243896
    },
    {
      "epoch": 0.2851948237139496,
      "step": 500,
      "train/gate_loss": 89.84481811523438,
      "train/gate_loss_post": 18.748319625854492,
      "train/gate_loss_pre": 5.2348175048828125,
      "train/lm_loss": 1.143736720085144
    },
    {
      "epoch": 0.2851948237139496,
      "step": 500,
      "train/gate_loss": 89.39913940429688,
      "train/gate_loss_post": 18.690067291259766,
      "train/gate_loss_pre": 5.201900005340576,
      "train/lm_loss": 1.2748481035232544
    },
    {
      "epoch": 0.2851948237139496,
      "step": 500,
      "train/gate_loss": 90.05138397216797,
      "train/gate_loss_post": 18.731355667114258,
      "train/gate_loss_pre": 5.258867263793945,
      "train/lm_loss": 1.0449320077896118
    },
    {
      "epoch": 0.3422337884567395,
      "grad_norm": NaN,
      "learning_rate": 0.00022585150332236082,
      "loss": 1729.8238,
      "step": 600
    },
    {
      "epoch": 0.3422337884567395,
      "step": 600,
      "train/gate_loss": 90.83132934570312,
      "train/gate_loss_post": 18.720096588134766,
      "train/gate_loss_pre": 5.339113235473633,
      "train/lm_loss": 1.0951054096221924
    },
    {
      "epoch": 0.3422337884567395,
      "step": 600,
      "train/gate_loss": 91.38807678222656,
      "train/gate_loss_post": 18.773649215698242,
      "train/gate_loss_pre": 5.384078025817871,
      "train/lm_loss": 1.1326059103012085
    },
    {
      "epoch": 0.3422337884567395,
      "step": 600,
      "train/gate_loss": 91.23017883300781,
      "train/gate_loss_post": 18.742122650146484,
      "train/gate_loss_pre": 5.374593734741211,
      "train/lm_loss": 1.0724740028381348
    },
    {
      "epoch": 0.3422337884567395,
      "step": 600,
      "train/gate_loss": 92.50784301757812,
      "train/gate_loss_post": 18.914081573486328,
      "train/gate_loss_pre": 5.467967510223389,
      "train/lm_loss": 1.2280502319335938
    },
    {
      "epoch": 0.3422337884567395,
      "step": 600,
      "train/gate_loss": 91.2461166381836,
      "train/gate_loss_post": 18.79244613647461,
      "train/gate_loss_pre": 5.366122245788574,
      "train/lm_loss": 0.9789198040962219
    },
    {
      "epoch": 0.3422337884567395,
      "step": 600,
      "train/gate_loss": 90.95923614501953,
      "train/gate_loss_post": 18.770490646362305,
      "train/gate_loss_pre": 5.341825485229492,
      "train/lm_loss": 1.0797710418701172
    },
    {
      "epoch": 0.3422337884567395,
      "step": 600,
      "train/gate_loss": 91.22019958496094,
      "train/gate_loss_post": 18.795486450195312,
      "train/gate_loss_pre": 5.3629231452941895,
      "train/lm_loss": 1.2094722986221313
    },
    {
      "epoch": 0.3422337884567395,
      "step": 600,
      "train/gate_loss": 91.38458251953125,
      "train/gate_loss_post": 18.749956130981445,
      "train/gate_loss_pre": 5.388467311859131,
      "train/lm_loss": 1.1157758235931396
    },
    {
      "epoch": 0.3422337884567395,
      "step": 600,
      "train/gate_loss": 91.4705810546875,
      "train/gate_loss_post": 18.83620834350586,
      "train/gate_loss_pre": 5.37981653213501,
      "train/lm_loss": 1.1898090839385986
    },
    {
      "epoch": 0.3422337884567395,
      "step": 600,
      "train/gate_loss": 91.65385437011719,
      "train/gate_loss_post": 18.87152862548828,
      "train/gate_loss_pre": 5.391079902648926,
      "train/lm_loss": 1.2107690572738647
    },
    {
      "epoch": 0.3422337884567395,
      "step": 600,
      "train/gate_loss": 92.26667785644531,
      "train/gate_loss_post": 19.00058937072754,
      "train/gate_loss_pre": 5.426549911499023,
      "train/lm_loss": 1.1503175497055054
    },
    {
      "epoch": 0.3422337884567395,
      "step": 600,
      "train/gate_loss": 91.033203125,
      "train/gate_loss_post": 18.662302017211914,
      "train/gate_loss_pre": 5.3708600997924805,
      "train/lm_loss": 0.9842086434364319
    },
    {
      "epoch": 0.3422337884567395,
      "step": 600,
      "train/gate_loss": 90.71195983886719,
      "train/gate_loss_post": 18.763761520385742,
      "train/gate_loss_pre": 5.31844425201416,
      "train/lm_loss": 1.115149736404419
    },
    {
      "epoch": 0.3422337884567395,
      "step": 600,
      "train/gate_loss": 90.92710876464844,
      "train/gate_loss_post": 18.750699996948242,
      "train/gate_loss_pre": 5.342570781707764,
      "train/lm_loss": 1.2509735822677612
    },
    {
      "epoch": 0.3422337884567395,
      "step": 600,
      "train/gate_loss": 90.76885986328125,
      "train/gate_loss_post": 18.633808135986328,
      "train/gate_loss_pre": 5.350124359130859,
      "train/lm_loss": 1.135980248451233
    },
    {
      "epoch": 0.3422337884567395,
      "step": 600,
      "train/gate_loss": 90.82949829101562,
      "train/gate_loss_post": 18.635128021240234,
      "train/gate_loss_pre": 5.355924606323242,
      "train/lm_loss": 1.1015245914459229
    },
    {
      "epoch": 0.39927275319952943,
      "grad_norm": 0.7277331352233887,
      "learning_rate": 0.00020284274559777846,
      "loss": 1536.8056,
      "step": 700
    },
    {
      "epoch": 0.39927275319952943,
      "step": 700,
      "train/gate_loss": 90.78562927246094,
      "train/gate_loss_post": 18.60165023803711,
      "train/gate_loss_pre": 5.358232498168945,
      "train/lm_loss": 1.1541721820831299
    },
    {
      "epoch": 0.39927275319952943,
      "step": 700,
      "train/gate_loss": 90.54707336425781,
      "train/gate_loss_post": 18.8748836517334,
      "train/gate_loss_pre": 5.279730796813965,
      "train/lm_loss": 1.444748044013977
    },
    {
      "epoch": 0.39927275319952943,
      "step": 700,
      "train/gate_loss": 91.58287048339844,
      "train/gate_loss_post": 18.80702018737793,
      "train/gate_loss_pre": 5.3968825340271,
      "train/lm_loss": 1.121008276939392
    },
    {
      "epoch": 0.39927275319952943,
      "step": 700,
      "train/gate_loss": 91.18367767333984,
      "train/gate_loss_post": 18.640995025634766,
      "train/gate_loss_pre": 5.3901686668396,
      "train/lm_loss": 1.153275966644287
    },
    {
      "epoch": 0.39927275319952943,
      "step": 700,
      "train/gate_loss": 91.01345825195312,
      "train/gate_loss_post": 18.652591705322266,
      "train/gate_loss_pre": 5.3708271980285645,
      "train/lm_loss": 1.1246695518493652
    },
    {
      "epoch": 0.39927275319952943,
      "step": 700,
      "train/gate_loss": 90.67362976074219,
      "train/gate_loss_post": 18.71891212463379,
      "train/gate_loss_pre": 5.323580741882324,
      "train/lm_loss": 1.0495134592056274
    },
    {
      "epoch": 0.39927275319952943,
      "step": 700,
      "train/gate_loss": 91.89389038085938,
      "train/gate_loss_post": 18.811471939086914,
      "train/gate_loss_pre": 5.427094459533691,
      "train/lm_loss": 1.3771253824234009
    },
    {
      "epoch": 0.39927275319952943,
      "step": 700,
      "train/gate_loss": 91.49629211425781,
      "train/gate_loss_post": 18.756946563720703,
      "train/gate_loss_pre": 5.398239612579346,
      "train/lm_loss": 1.175488829612732
    },
    {
      "epoch": 0.39927275319952943,
      "step": 700,
      "train/gate_loss": 91.35629272460938,
      "train/gate_loss_post": 18.844703674316406,
      "train/gate_loss_pre": 5.366688251495361,
      "train/lm_loss": 1.203412413597107
    },
    {
      "epoch": 0.39927275319952943,
      "step": 700,
      "train/gate_loss": 91.40556335449219,
      "train/gate_loss_post": 18.75667953491211,
      "train/gate_loss_pre": 5.389220714569092,
      "train/lm_loss": 1.0664119720458984
    },
    {
      "epoch": 0.39927275319952943,
      "step": 700,
      "train/gate_loss": 90.4899673461914,
      "train/gate_loss_post": 18.71792984008789,
      "train/gate_loss_pre": 5.305410861968994,
      "train/lm_loss": 1.266525387763977
    },
    {
      "epoch": 0.39927275319952943,
      "step": 700,
      "train/gate_loss": 91.70494842529297,
      "train/gate_loss_post": 18.81125831604004,
      "train/gate_loss_pre": 5.408243179321289,
      "train/lm_loss": 1.0716502666473389
    },
    {
      "epoch": 0.39927275319952943,
      "step": 700,
      "train/gate_loss": 91.94490051269531,
      "train/gate_loss_post": 18.84726333618164,
      "train/gate_loss_pre": 5.425036907196045,
      "train/lm_loss": 1.2153514623641968
    },
    {
      "epoch": 0.39927275319952943,
      "step": 700,
      "train/gate_loss": 91.42616271972656,
      "train/gate_loss_post": 18.718364715576172,
      "train/gate_loss_pre": 5.3989434242248535,
      "train/lm_loss": 1.3397276401519775
    },
    {
      "epoch": 0.39927275319952943,
      "step": 700,
      "train/gate_loss": 91.76458740234375,
      "train/gate_loss_post": 18.904388427734375,
      "train/gate_loss_pre": 5.395580768585205,
      "train/lm_loss": 1.2640339136123657
    },
    {
      "epoch": 0.39927275319952943,
      "step": 700,
      "train/gate_loss": 91.58633422851562,
      "train/gate_loss_post": 18.813615798950195,
      "train/gate_loss_pre": 5.395910739898682,
      "train/lm_loss": 1.040794849395752
    },
    {
      "epoch": 0.45631171794231934,
      "grad_norm": 0.8193921446800232,
      "learning_rate": 0.00017698743396994335,
      "loss": 1462.4402,
      "step": 800
    },
    {
      "epoch": 0.45631171794231934,
      "step": 800,
      "train/gate_loss": 92.26026916503906,
      "train/gate_loss_post": 19.056901931762695,
      "train/gate_loss_pre": 5.414647102355957,
      "train/lm_loss": 1.237480878829956
    },
    {
      "epoch": 0.45631171794231934,
      "step": 800,
      "train/gate_loss": 92.23503875732422,
      "train/gate_loss_post": 18.800025939941406,
      "train/gate_loss_pre": 5.463498592376709,
      "train/lm_loss": 1.1419920921325684
    },
    {
      "epoch": 0.45631171794231934,
      "step": 800,
      "train/gate_loss": 91.86410522460938,
      "train/gate_loss_post": 18.896331787109375,
      "train/gate_loss_pre": 5.407144546508789,
      "train/lm_loss": 1.0477627515792847
    },
    {
      "epoch": 0.45631171794231934,
      "step": 800,
      "train/gate_loss": 92.23751831054688,
      "train/gate_loss_post": 18.84709930419922,
      "train/gate_loss_pre": 5.454331874847412,
      "train/lm_loss": 1.183019995689392
    },
    {
      "epoch": 0.45631171794231934,
      "step": 800,
      "train/gate_loss": 92.52788543701172,
      "train/gate_loss_post": 18.87679100036621,
      "train/gate_loss_pre": 5.47743034362793,
      "train/lm_loss": 1.2082958221435547
    },
    {
      "epoch": 0.45631171794231934,
      "step": 800,
      "train/gate_loss": 92.082275390625,
      "train/gate_loss_post": 18.871355056762695,
      "train/gate_loss_pre": 5.433957099914551,
      "train/lm_loss": 1.1685370206832886
    },
    {
      "epoch": 0.45631171794231934,
      "step": 800,
      "train/gate_loss": 92.45372009277344,
      "train/gate_loss_post": 18.846460342407227,
      "train/gate_loss_pre": 5.47607946395874,
      "train/lm_loss": 1.333703875541687
    },
    {
      "epoch": 0.45631171794231934,
      "step": 800,
      "train/gate_loss": 92.36997985839844,
      "train/gate_loss_post": 18.91231918334961,
      "train/gate_loss_pre": 5.454534530639648,
      "train/lm_loss": 1.3434827327728271
    },
    {
      "epoch": 0.45631171794231934,
      "step": 800,
      "train/gate_loss": 92.09292602539062,
      "train/gate_loss_post": 18.926546096801758,
      "train/gate_loss_pre": 5.423983573913574,
      "train/lm_loss": 1.2574560642242432
    },
    {
      "epoch": 0.45631171794231934,
      "step": 800,
      "train/gate_loss": 92.79014587402344,
      "train/gate_loss_post": 18.9733829498291,
      "train/gate_loss_pre": 5.484338283538818,
      "train/lm_loss": 1.1937615871429443
    },
    {
      "epoch": 0.45631171794231934,
      "step": 800,
      "train/gate_loss": 91.97794342041016,
      "train/gate_loss_post": 18.788150787353516,
      "train/gate_loss_pre": 5.440164089202881,
      "train/lm_loss": 1.1819252967834473
    },
    {
      "epoch": 0.45631171794231934,
      "step": 800,
      "train/gate_loss": 93.02777099609375,
      "train/gate_loss_post": 18.988176345825195,
      "train/gate_loss_pre": 5.505141258239746,
      "train/lm_loss": 1.3156923055648804
    },
    {
      "epoch": 0.45631171794231934,
      "step": 800,
      "train/gate_loss": 91.94398498535156,
      "train/gate_loss_post": 18.733766555786133,
      "train/gate_loss_pre": 5.447645664215088,
      "train/lm_loss": 1.1928317546844482
    },
    {
      "epoch": 0.45631171794231934,
      "step": 800,
      "train/gate_loss": 92.4415512084961,
      "train/gate_loss_post": 18.884803771972656,
      "train/gate_loss_pre": 5.467194557189941,
      "train/lm_loss": 2.071682929992676
    },
    {
      "epoch": 0.45631171794231934,
      "step": 800,
      "train/gate_loss": 92.20698547363281,
      "train/gate_loss_post": 18.879512786865234,
      "train/gate_loss_pre": 5.444796085357666,
      "train/lm_loss": 1.2605911493301392
    },
    {
      "epoch": 0.45631171794231934,
      "step": 800,
      "train/gate_loss": 92.98573303222656,
      "train/gate_loss_post": 19.030988693237305,
      "train/gate_loss_pre": 5.492375373840332,
      "train/lm_loss": 1.2150331735610962
    },
    {
      "epoch": 0.5133506826851093,
      "grad_norm": 0.4715077877044678,
      "learning_rate": 0.0001502686651345947,
      "loss": 1486.8484,
      "step": 900
    },
    {
      "epoch": 0.5133506826851093,
      "step": 900,
      "train/gate_loss": 92.80426788330078,
      "train/gate_loss_post": 18.89629554748535,
      "train/gate_loss_pre": 5.5011677742004395,
      "train/lm_loss": 1.1265876293182373
    },
    {
      "epoch": 0.5133506826851093,
      "step": 900,
      "train/gate_loss": 90.97886657714844,
      "train/gate_loss_post": 18.786029815673828,
      "train/gate_loss_pre": 5.3406805992126465,
      "train/lm_loss": 1.086159586906433
    },
    {
      "epoch": 0.5133506826851093,
      "step": 900,
      "train/gate_loss": 91.97694396972656,
      "train/gate_loss_post": 18.813148498535156,
      "train/gate_loss_pre": 5.435064315795898,
      "train/lm_loss": 1.1556559801101685
    },
    {
      "epoch": 0.5133506826851093,
      "step": 900,
      "train/gate_loss": 92.23019409179688,
      "train/gate_loss_post": 18.847745895385742,
      "train/gate_loss_pre": 5.453470230102539,
      "train/lm_loss": 1.169100046157837
    },
    {
      "epoch": 0.5133506826851093,
      "step": 900,
      "train/gate_loss": 92.61874389648438,
      "train/gate_loss_post": 18.888364791870117,
      "train/gate_loss_pre": 5.484201908111572,
      "train/lm_loss": 1.093690276145935
    },
    {
      "epoch": 0.5133506826851093,
      "step": 900,
      "train/gate_loss": 91.85073852539062,
      "train/gate_loss_post": 18.853376388549805,
      "train/gate_loss_pre": 5.414398193359375,
      "train/lm_loss": 1.1622288227081299
    },
    {
      "epoch": 0.5133506826851093,
      "step": 900,
      "train/gate_loss": 91.68448638916016,
      "train/gate_loss_post": 18.887073516845703,
      "train/gate_loss_pre": 5.391034126281738,
      "train/lm_loss": 1.180703043937683
    },
    {
      "epoch": 0.5133506826851093,
      "step": 900,
      "train/gate_loss": 93.13386535644531,
      "train/gate_loss_post": 18.951412200927734,
      "train/gate_loss_pre": 5.523104190826416,
      "train/lm_loss": 1.1478325128555298
    },
    {
      "epoch": 0.5133506826851093,
      "step": 900,
      "train/gate_loss": 91.19895935058594,
      "train/gate_loss_post": 18.78849983215332,
      "train/gate_loss_pre": 5.3621954917907715,
      "train/lm_loss": 0.9686160683631897
    },
    {
      "epoch": 0.5133506826851093,
      "step": 900,
      "train/gate_loss": 92.38824462890625,
      "train/gate_loss_post": 18.975597381591797,
      "train/gate_loss_pre": 5.443705081939697,
      "train/lm_loss": 1.2447113990783691
    },
    {
      "epoch": 0.5133506826851093,
      "step": 900,
      "train/gate_loss": 92.27297973632812,
      "train/gate_loss_post": 18.913301467895508,
      "train/gate_loss_pre": 5.444637298583984,
      "train/lm_loss": 1.2732702493667603
    },
    {
      "epoch": 0.5133506826851093,
      "step": 900,
      "train/gate_loss": 92.59207153320312,
      "train/gate_loss_post": 18.942401885986328,
      "train/gate_loss_pre": 5.47072696685791,
      "train/lm_loss": 1.325950264930725
    },
    {
      "epoch": 0.5133506826851093,
      "step": 900,
      "train/gate_loss": 93.1107177734375,
      "train/gate_loss_post": 19.022314071655273,
      "train/gate_loss_pre": 5.506608963012695,
      "train/lm_loss": 1.3670661449432373
    },
    {
      "epoch": 0.5133506826851093,
      "step": 900,
      "train/gate_loss": 92.55021667480469,
      "train/gate_loss_post": 18.863840103149414,
      "train/gate_loss_pre": 5.4822540283203125,
      "train/lm_loss": 1.181179165840149
    },
    {
      "epoch": 0.5133506826851093,
      "step": 900,
      "train/gate_loss": 91.7751235961914,
      "train/gate_loss_post": 18.808320999145508,
      "train/gate_loss_pre": 5.415848255157471,
      "train/lm_loss": 1.229018211364746
    },
    {
      "epoch": 0.5133506826851093,
      "step": 900,
      "train/gate_loss": 92.56967163085938,
      "train/gate_loss_post": 18.872241973876953,
      "train/gate_loss_pre": 5.482519149780273,
      "train/lm_loss": 1.0878593921661377
    },
    {
      "epoch": 0.5703896474278992,
      "grad_norm": 0.3969462811946869,
      "learning_rate": 0.00012354130041543102,
      "loss": 1486.5223,
      "step": 1000
    },
    {
      "epoch": 0.5703896474278992,
      "step": 1000,
      "train/gate_loss": 92.7139892578125,
      "train/gate_loss_post": 19.046411514282227,
      "train/gate_loss_pre": 5.4621171951293945,
      "train/lm_loss": 1.1952348947525024
    },
    {
      "epoch": 0.5703896474278992,
      "step": 1000,
      "train/gate_loss": 91.9251480102539,
      "train/gate_loss_post": 18.885507583618164,
      "train/gate_loss_pre": 5.4154133796691895,
      "train/lm_loss": 1.1281648874282837
    },
    {
      "epoch": 0.5703896474278992,
      "step": 1000,
      "train/gate_loss": 92.84076690673828,
      "train/gate_loss_post": 18.971223831176758,
      "train/gate_loss_pre": 5.489831924438477,
      "train/lm_loss": 1.2064924240112305
    },
    {
      "epoch": 0.5703896474278992,
      "step": 1000,
      "train/gate_loss": 92.09757995605469,
      "train/gate_loss_post": 18.808748245239258,
      "train/gate_loss_pre": 5.4480085372924805,
      "train/lm_loss": 1.253547191619873
    },
    {
      "epoch": 0.5703896474278992,
      "step": 1000,
      "train/gate_loss": 91.87149047851562,
      "train/gate_loss_post": 18.84899139404297,
      "train/gate_loss_pre": 5.4173502922058105,
      "train/lm_loss": 1.127891182899475
    },
    {
      "epoch": 0.5703896474278992,
      "step": 1000,
      "train/gate_loss": 92.20455169677734,
      "train/gate_loss_post": 18.924659729003906,
      "train/gate_loss_pre": 5.43552303314209,
      "train/lm_loss": 1.1686816215515137
    },
    {
      "epoch": 0.5703896474278992,
      "step": 1000,
      "train/gate_loss": 92.01077270507812,
      "train/gate_loss_post": 18.887680053710938,
      "train/gate_loss_pre": 5.423541069030762,
      "train/lm_loss": 1.1934454441070557
    },
    {
      "epoch": 0.5703896474278992,
      "step": 1000,
      "train/gate_loss": 92.32290649414062,
      "train/gate_loss_post": 18.807109832763672,
      "train/gate_loss_pre": 5.470869064331055,
      "train/lm_loss": 1.121302843093872
    },
    {
      "epoch": 0.5703896474278992,
      "step": 1000,
      "train/gate_loss": 93.03129577636719,
      "train/gate_loss_post": 19.006072998046875,
      "train/gate_loss_pre": 5.5019145011901855,
      "train/lm_loss": 1.2645249366760254
    },
    {
      "epoch": 0.5703896474278992,
      "step": 1000,
      "train/gate_loss": 92.3721923828125,
      "train/gate_loss_post": 18.850664138793945,
      "train/gate_loss_pre": 5.467086315155029,
      "train/lm_loss": 1.2556720972061157
    },
    {
      "epoch": 0.5703896474278992,
      "step": 1000,
      "train/gate_loss": 92.61500549316406,
      "train/gate_loss_post": 18.778345108032227,
      "train/gate_loss_pre": 5.505831718444824,
      "train/lm_loss": 1.075600266456604
    },
    {
      "epoch": 0.5703896474278992,
      "step": 1000,
      "train/gate_loss": 92.21560668945312,
      "train/gate_loss_post": 18.84661293029785,
      "train/gate_loss_pre": 5.452238082885742,
      "train/lm_loss": 1.1997804641723633
    },
    {
      "epoch": 0.5703896474278992,
      "step": 1000,
      "train/gate_loss": 92.49462890625,
      "train/gate_loss_post": 18.88827896118164,
      "train/gate_loss_pre": 5.47180700302124,
      "train/lm_loss": 1.3275257349014282
    },
    {
      "epoch": 0.5703896474278992,
      "step": 1000,
      "train/gate_loss": 91.98353576660156,
      "train/gate_loss_post": 18.835716247558594,
      "train/gate_loss_pre": 5.431210041046143,
      "train/lm_loss": 1.1299761533737183
    },
    {
      "epoch": 0.5703896474278992,
      "step": 1000,
      "train/gate_loss": 92.13189697265625,
      "train/gate_loss_post": 18.963302612304688,
      "train/gate_loss_pre": 5.420529365539551,
      "train/lm_loss": 1.2428101301193237
    },
    {
      "epoch": 0.5703896474278992,
      "step": 1000,
      "train/gate_loss": 91.44198608398438,
      "train/gate_loss_post": 18.77225112915039,
      "train/gate_loss_pre": 5.389748573303223,
      "train/lm_loss": 1.1883805990219116
    },
    {
      "epoch": 0.6274286121706891,
      "grad_norm": 0.32863205671310425,
      "learning_rate": 9.766047615962429e-05,
      "loss": 1485.3205,
      "step": 1100
    },
    {
      "epoch": 0.6274286121706891,
      "step": 1100,
      "train/gate_loss": 92.3104248046875,
      "train/gate_loss_post": 18.81551742553711,
      "train/gate_loss_pre": 5.4679388999938965,
      "train/lm_loss": 1.1406477689743042
    },
    {
      "epoch": 0.6274286121706891,
      "step": 1100,
      "train/gate_loss": 91.73805236816406,
      "train/gate_loss_post": 18.784809112548828,
      "train/gate_loss_pre": 5.416843414306641,
      "train/lm_loss": 1.2069185972213745
    },
    {
      "epoch": 0.6274286121706891,
      "step": 1100,
      "train/gate_loss": 93.34133911132812,
      "train/gate_loss_post": 18.98455810546875,
      "train/gate_loss_pre": 5.537222385406494,
      "train/lm_loss": 1.0480711460113525
    },
    {
      "epoch": 0.6274286121706891,
      "step": 1100,
      "train/gate_loss": 91.36079406738281,
      "train/gate_loss_post": 18.75943374633789,
      "train/gate_loss_pre": 5.38419246673584,
      "train/lm_loss": 1.2207980155944824
    },
    {
      "epoch": 0.6274286121706891,
      "step": 1100,
      "train/gate_loss": 92.71133422851562,
      "train/gate_loss_post": 18.886747360229492,
      "train/gate_loss_pre": 5.493784427642822,
      "train/lm_loss": 1.2738173007965088
    },
    {
      "epoch": 0.6274286121706891,
      "step": 1100,
      "train/gate_loss": 92.38481903076172,
      "train/gate_loss_post": 18.970670700073242,
      "train/gate_loss_pre": 5.444347858428955,
      "train/lm_loss": 1.2957850694656372
    },
    {
      "epoch": 0.6274286121706891,
      "step": 1100,
      "train/gate_loss": 92.00572204589844,
      "train/gate_loss_post": 18.85501480102539,
      "train/gate_loss_pre": 5.429569721221924,
      "train/lm_loss": 1.376542568206787
    },
    {
      "epoch": 0.6274286121706891,
      "step": 1100,
      "train/gate_loss": 92.44744873046875,
      "train/gate_loss_post": 18.82451629638672,
      "train/gate_loss_pre": 5.479841709136963,
      "train/lm_loss": 1.1095752716064453
    },
    {
      "epoch": 0.6274286121706891,
      "step": 1100,
      "train/gate_loss": 92.21244812011719,
      "train/gate_loss_post": 18.80120086669922,
      "train/gate_loss_pre": 5.461004734039307,
      "train/lm_loss": 1.1916115283966064
    },
    {
      "epoch": 0.6274286121706891,
      "step": 1100,
      "train/gate_loss": 92.58826446533203,
      "train/gate_loss_post": 19.013713836669922,
      "train/gate_loss_pre": 5.45608377456665,
      "train/lm_loss": 1.1153709888458252
    },
    {
      "epoch": 0.6274286121706891,
      "step": 1100,
      "train/gate_loss": 91.81968688964844,
      "train/gate_loss_post": 18.987993240356445,
      "train/gate_loss_pre": 5.384369850158691,
      "train/lm_loss": 1.1383452415466309
    },
    {
      "epoch": 0.6274286121706891,
      "step": 1100,
      "train/gate_loss": 92.38597869873047,
      "train/gate_loss_post": 18.882843017578125,
      "train/gate_loss_pre": 5.462029457092285,
      "train/lm_loss": 1.052088737487793
    },
    {
      "epoch": 0.6274286121706891,
      "step": 1100,
      "train/gate_loss": 92.23957824707031,
      "train/gate_loss_post": 18.80815315246582,
      "train/gate_loss_pre": 5.462327003479004,
      "train/lm_loss": 1.0774222612380981
    },
    {
      "epoch": 0.6274286121706891,
      "step": 1100,
      "train/gate_loss": 91.82733154296875,
      "train/gate_loss_post": 18.750886917114258,
      "train/gate_loss_pre": 5.43255615234375,
      "train/lm_loss": 1.1813267469406128
    },
    {
      "epoch": 0.6274286121706891,
      "step": 1100,
      "train/gate_loss": 92.42228698730469,
      "train/gate_loss_post": 18.828845977783203,
      "train/gate_loss_pre": 5.476459980010986,
      "train/lm_loss": 1.106879472732544
    },
    {
      "epoch": 0.6274286121706891,
      "step": 1100,
      "train/gate_loss": 91.60018920898438,
      "train/gate_loss_post": 18.762298583984375,
      "train/gate_loss_pre": 5.407558917999268,
      "train/lm_loss": 1.1727814674377441
    },
    {
      "epoch": 0.684467576913479,
      "grad_norm": 0.35031649470329285,
      "learning_rate": 7.345424383234722e-05,
      "loss": 1489.0631,
      "step": 1200
    },
    {
      "epoch": 0.684467576913479,
      "step": 1200,
      "train/gate_loss": 92.44853210449219,
      "train/gate_loss_post": 18.85542106628418,
      "train/gate_loss_pre": 5.473769187927246,
      "train/lm_loss": 1.1685587167739868
    },
    {
      "epoch": 0.684467576913479,
      "step": 1200,
      "train/gate_loss": 92.22134399414062,
      "train/gate_loss_post": 18.782228469848633,
      "train/gate_loss_pre": 5.465688228607178,
      "train/lm_loss": 1.1549413204193115
    },
    {
      "epoch": 0.684467576913479,
      "step": 1200,
      "train/gate_loss": 91.85182189941406,
      "train/gate_loss_post": 18.846776962280273,
      "train/gate_loss_pre": 5.415826320648193,
      "train/lm_loss": 1.1786324977874756
    },
    {
      "epoch": 0.684467576913479,
      "step": 1200,
      "train/gate_loss": 92.00042724609375,
      "train/gate_loss_post": 18.81392478942871,
      "train/gate_loss_pre": 5.437257289886475,
      "train/lm_loss": 1.1891425848007202
    },
    {
      "epoch": 0.684467576913479,
      "step": 1200,
      "train/gate_loss": 92.66490173339844,
      "train/gate_loss_post": 18.957168579101562,
      "train/gate_loss_pre": 5.4750566482543945,
      "train/lm_loss": 1.2993241548538208
    },
    {
      "epoch": 0.684467576913479,
      "step": 1200,
      "train/gate_loss": 92.0150146484375,
      "train/gate_loss_post": 18.815784454345703,
      "train/gate_loss_pre": 5.438344478607178,
      "train/lm_loss": 1.124350905418396
    },
    {
      "epoch": 0.684467576913479,
      "step": 1200,
      "train/gate_loss": 92.23426818847656,
      "train/gate_loss_post": 18.88849449157715,
      "train/gate_loss_pre": 5.445727825164795,
      "train/lm_loss": 1.2750152349472046
    },
    {
      "epoch": 0.684467576913479,
      "step": 1200,
      "train/gate_loss": 91.8764877319336,
      "train/gate_loss_post": 18.776432037353516,
      "train/gate_loss_pre": 5.4323625564575195,
      "train/lm_loss": 1.241494059562683
    },
    {
      "epoch": 0.684467576913479,
      "step": 1200,
      "train/gate_loss": 92.22966003417969,
      "train/gate_loss_post": 18.84745979309082,
      "train/gate_loss_pre": 5.453474044799805,
      "train/lm_loss": 1.2957570552825928
    },
    {
      "epoch": 0.684467576913479,
      "step": 1200,
      "train/gate_loss": 92.39132690429688,
      "train/gate_loss_post": 18.82305908203125,
      "train/gate_loss_pre": 5.474520683288574,
      "train/lm_loss": 1.2799516916275024
    },
    {
      "epoch": 0.684467576913479,
      "step": 1200,
      "train/gate_loss": 92.40684509277344,
      "train/gate_loss_post": 18.887853622436523,
      "train/gate_loss_pre": 5.463113307952881,
      "train/lm_loss": 1.2619044780731201
    },
    {
      "epoch": 0.684467576913479,
      "step": 1200,
      "train/gate_loss": 91.79202270507812,
      "train/gate_loss_post": 18.856693267822266,
      "train/gate_loss_pre": 5.407863616943359,
      "train/lm_loss": 1.229362964630127
    },
    {
      "epoch": 0.684467576913479,
      "step": 1200,
      "train/gate_loss": 92.381103515625,
      "train/gate_loss_post": 18.82171058654785,
      "train/gate_loss_pre": 5.473768711090088,
      "train/lm_loss": 1.193161964416504
    },
    {
      "epoch": 0.684467576913479,
      "step": 1200,
      "train/gate_loss": 91.50100708007812,
      "train/gate_loss_post": 18.771156311035156,
      "train/gate_loss_pre": 5.395869731903076,
      "train/lm_loss": 1.0861871242523193
    },
    {
      "epoch": 0.684467576913479,
      "step": 1200,
      "train/gate_loss": 91.48715209960938,
      "train/gate_loss_post": 18.720808029174805,
      "train/gate_loss_pre": 5.4045538902282715,
      "train/lm_loss": 1.1450526714324951
    },
    {
      "epoch": 0.684467576913479,
      "step": 1200,
      "train/gate_loss": 93.01803588867188,
      "train/gate_loss_post": 18.899850845336914,
      "train/gate_loss_pre": 5.521833896636963,
      "train/lm_loss": 1.2567272186279297
    },
    {
      "epoch": 0.741506541656269,
      "grad_norm": 0.3625767230987549,
      "learning_rate": 5.169707668633716e-05,
      "loss": 1489.4552,
      "step": 1300
    },
    {
      "epoch": 0.741506541656269,
      "step": 1300,
      "train/gate_loss": 91.9892578125,
      "train/gate_loss_post": 18.75593376159668,
      "train/gate_loss_pre": 5.4477386474609375,
      "train/lm_loss": 1.1240822076797485
    },
    {
      "epoch": 0.741506541656269,
      "step": 1300,
      "train/gate_loss": 91.67964935302734,
      "train/gate_loss_post": 18.672935485839844,
      "train/gate_loss_pre": 5.433377742767334,
      "train/lm_loss": 1.28916597366333
    },
    {
      "epoch": 0.741506541656269,
      "step": 1300,
      "train/gate_loss": 91.2738037109375,
      "train/gate_loss_post": 18.79178810119629,
      "train/gate_loss_pre": 5.369022369384766,
      "train/lm_loss": 1.1379286050796509
    },
    {
      "epoch": 0.741506541656269,
      "step": 1300,
      "train/gate_loss": 92.74818420410156,
      "train/gate_loss_post": 18.792747497558594,
      "train/gate_loss_pre": 5.516269207000732,
      "train/lm_loss": 1.15227472782135
    },
    {
      "epoch": 0.741506541656269,
      "step": 1300,
      "train/gate_loss": 91.50044250488281,
      "train/gate_loss_post": 18.753156661987305,
      "train/gate_loss_pre": 5.399413108825684,
      "train/lm_loss": 1.152900218963623
    },
    {
      "epoch": 0.741506541656269,
      "step": 1300,
      "train/gate_loss": 92.48943328857422,
      "train/gate_loss_post": 18.841270446777344,
      "train/gate_loss_pre": 5.48068904876709,
      "train/lm_loss": 1.0512995719909668
    },
    {
      "epoch": 0.741506541656269,
      "step": 1300,
      "train/gate_loss": 92.52763366699219,
      "train/gate_loss_post": 18.86376953125,
      "train/gate_loss_pre": 5.480009078979492,
      "train/lm_loss": 1.196552038192749
    },
    {
      "epoch": 0.741506541656269,
      "step": 1300,
      "train/gate_loss": 92.5853271484375,
      "train/gate_loss_post": 18.814409255981445,
      "train/gate_loss_pre": 5.495650291442871,
      "train/lm_loss": 1.0720967054367065
    },
    {
      "epoch": 0.741506541656269,
      "step": 1300,
      "train/gate_loss": 92.43981170654297,
      "train/gate_loss_post": 18.860580444335938,
      "train/gate_loss_pre": 5.471865177154541,
      "train/lm_loss": 1.2934857606887817
    },
    {
      "epoch": 0.741506541656269,
      "step": 1300,
      "train/gate_loss": 92.76581573486328,
      "train/gate_loss_post": 18.966466903686523,
      "train/gate_loss_pre": 5.483288288116455,
      "train/lm_loss": 1.1922944784164429
    },
    {
      "epoch": 0.741506541656269,
      "step": 1300,
      "train/gate_loss": 92.09901428222656,
      "train/gate_loss_post": 18.783628463745117,
      "train/gate_loss_pre": 5.453176021575928,
      "train/lm_loss": 1.1469370126724243
    },
    {
      "epoch": 0.741506541656269,
      "step": 1300,
      "train/gate_loss": 92.39793395996094,
      "train/gate_loss_post": 18.78026580810547,
      "train/gate_loss_pre": 5.483740329742432,
      "train/lm_loss": 1.1237711906433105
    },
    {
      "epoch": 0.741506541656269,
      "step": 1300,
      "train/gate_loss": 92.38630676269531,
      "train/gate_loss_post": 18.802696228027344,
      "train/gate_loss_pre": 5.478091716766357,
      "train/lm_loss": 1.225725531578064
    },
    {
      "epoch": 0.741506541656269,
      "step": 1300,
      "train/gate_loss": 92.37840270996094,
      "train/gate_loss_post": 18.751052856445312,
      "train/gate_loss_pre": 5.487629413604736,
      "train/lm_loss": 1.2352215051651
    },
    {
      "epoch": 0.741506541656269,
      "step": 1300,
      "train/gate_loss": 91.50906372070312,
      "train/gate_loss_post": 18.826683044433594,
      "train/gate_loss_pre": 5.3855695724487305,
      "train/lm_loss": 1.1587934494018555
    },
    {
      "epoch": 0.741506541656269,
      "step": 1300,
      "train/gate_loss": 92.72412872314453,
      "train/gate_loss_post": 18.95553970336914,
      "train/gate_loss_pre": 5.481305122375488,
      "train/lm_loss": 1.269287347793579
    },
    {
      "epoch": 0.7985455063990589,
      "grad_norm": 0.37123429775238037,
      "learning_rate": 3.308509065496589e-05,
      "loss": 1489.8483,
      "step": 1400
    },
    {
      "epoch": 0.7985455063990589,
      "step": 1400,
      "train/gate_loss": 92.60111236572266,
      "train/gate_loss_post": 18.807025909423828,
      "train/gate_loss_pre": 5.498705863952637,
      "train/lm_loss": 1.2082664966583252
    },
    {
      "epoch": 0.7985455063990589,
      "step": 1400,
      "train/gate_loss": 93.59883117675781,
      "train/gate_loss_post": 19.04498863220215,
      "train/gate_loss_pre": 5.550885200500488,
      "train/lm_loss": 1.2448304891586304
    },
    {
      "epoch": 0.7985455063990589,
      "step": 1400,
      "train/gate_loss": 93.139892578125,
      "train/gate_loss_post": 18.835113525390625,
      "train/gate_loss_pre": 5.546967029571533,
      "train/lm_loss": 1.2370322942733765
    },
    {
      "epoch": 0.7985455063990589,
      "step": 1400,
      "train/gate_loss": 92.22976684570312,
      "train/gate_loss_post": 18.797395706176758,
      "train/gate_loss_pre": 5.463497638702393,
      "train/lm_loss": 1.2236770391464233
    },
    {
      "epoch": 0.7985455063990589,
      "step": 1400,
      "train/gate_loss": 91.62034606933594,
      "train/gate_loss_post": 18.835338592529297,
      "train/gate_loss_pre": 5.394967079162598,
      "train/lm_loss": 1.1019998788833618
    },
    {
      "epoch": 0.7985455063990589,
      "step": 1400,
      "train/gate_loss": 91.91046905517578,
      "train/gate_loss_post": 18.824506759643555,
      "train/gate_loss_pre": 5.426145553588867,
      "train/lm_loss": 1.218124508857727
    },
    {
      "epoch": 0.7985455063990589,
      "step": 1400,
      "train/gate_loss": 92.65687561035156,
      "train/gate_loss_post": 18.821990966796875,
      "train/gate_loss_pre": 5.501289367675781,
      "train/lm_loss": 1.1728206872940063
    },
    {
      "epoch": 0.7985455063990589,
      "step": 1400,
      "train/gate_loss": 92.33110809326172,
      "train/gate_loss_post": 18.828996658325195,
      "train/gate_loss_pre": 5.467311382293701,
      "train/lm_loss": 1.1877193450927734
    },
    {
      "epoch": 0.7985455063990589,
      "step": 1400,
      "train/gate_loss": 92.46318054199219,
      "train/gate_loss_post": 18.758459091186523,
      "train/gate_loss_pre": 5.494626522064209,
      "train/lm_loss": 1.2128510475158691
    },
    {
      "epoch": 0.7985455063990589,
      "step": 1400,
      "train/gate_loss": 92.37396240234375,
      "train/gate_loss_post": 18.768312454223633,
      "train/gate_loss_pre": 5.483734130859375,
      "train/lm_loss": 1.163007140159607
    },
    {
      "epoch": 0.7985455063990589,
      "step": 1400,
      "train/gate_loss": 92.80299377441406,
      "train/gate_loss_post": 18.87598991394043,
      "train/gate_loss_pre": 5.505101203918457,
      "train/lm_loss": 1.302533507347107
    },
    {
      "epoch": 0.7985455063990589,
      "step": 1400,
      "train/gate_loss": 92.52996826171875,
      "train/gate_loss_post": 18.854463577270508,
      "train/gate_loss_pre": 5.4821038246154785,
      "train/lm_loss": 1.4106611013412476
    },
    {
      "epoch": 0.7985455063990589,
      "step": 1400,
      "train/gate_loss": 92.05345916748047,
      "train/gate_loss_post": 18.849346160888672,
      "train/gate_loss_pre": 5.435476779937744,
      "train/lm_loss": 1.1978671550750732
    },
    {
      "epoch": 0.7985455063990589,
      "step": 1400,
      "train/gate_loss": 92.01341247558594,
      "train/gate_loss_post": 18.849681854248047,
      "train/gate_loss_pre": 5.4314045906066895,
      "train/lm_loss": 1.0910059213638306
    },
    {
      "epoch": 0.7985455063990589,
      "step": 1400,
      "train/gate_loss": 92.63983154296875,
      "train/gate_loss_post": 18.80401611328125,
      "train/gate_loss_pre": 5.503180027008057,
      "train/lm_loss": 1.1322543621063232
    },
    {
      "epoch": 0.7985455063990589,
      "step": 1400,
      "train/gate_loss": 92.78140258789062,
      "train/gate_loss_post": 18.88878059387207,
      "train/gate_loss_pre": 5.5003838539123535,
      "train/lm_loss": 1.3512970209121704
    },
    {
      "epoch": 0.8555844711418488,
      "grad_norm": 0.3605785667896271,
      "learning_rate": 1.8213772271070355e-05,
      "loss": 1494.6117,
      "step": 1500
    },
    {
      "epoch": 0.8555844711418488,
      "step": 1500,
      "train/gate_loss": 92.85637664794922,
      "train/gate_loss_post": 18.806325912475586,
      "train/gate_loss_pre": 5.524372577667236,
      "train/lm_loss": 1.1819044351577759
    },
    {
      "epoch": 0.8555844711418488,
      "step": 1500,
      "train/gate_loss": 92.25521850585938,
      "train/gate_loss_post": 18.89348793029785,
      "train/gate_loss_pre": 5.446824073791504,
      "train/lm_loss": 1.1816844940185547
    },
    {
      "epoch": 0.8555844711418488,
      "step": 1500,
      "train/gate_loss": 93.11177825927734,
      "train/gate_loss_post": 19.037403106689453,
      "train/gate_loss_pre": 5.503697395324707,
      "train/lm_loss": 1.4175344705581665
    },
    {
      "epoch": 0.8555844711418488,
      "step": 1500,
      "train/gate_loss": 91.5552978515625,
      "train/gate_loss_post": 18.757253646850586,
      "train/gate_loss_pre": 5.404079437255859,
      "train/lm_loss": 1.1551495790481567
    },
    {
      "epoch": 0.8555844711418488,
      "step": 1500,
      "train/gate_loss": 92.77651977539062,
      "train/gate_loss_post": 18.8836727142334,
      "train/gate_loss_pre": 5.500917434692383,
      "train/lm_loss": 1.082252025604248
    },
    {
      "epoch": 0.8555844711418488,
      "step": 1500,
      "train/gate_loss": 91.9445571899414,
      "train/gate_loss_post": 18.765281677246094,
      "train/gate_loss_pre": 5.441399574279785,
      "train/lm_loss": 1.2999175786972046
    },
    {
      "epoch": 0.8555844711418488,
      "step": 1500,
      "train/gate_loss": 92.68587493896484,
      "train/gate_loss_post": 18.814584732055664,
      "train/gate_loss_pre": 5.505670547485352,
      "train/lm_loss": 1.335608720779419
    },
    {
      "epoch": 0.8555844711418488,
      "step": 1500,
      "train/gate_loss": 92.55201721191406,
      "train/gate_loss_post": 18.985153198242188,
      "train/gate_loss_pre": 5.458171367645264,
      "train/lm_loss": 1.2523995637893677
    },
    {
      "epoch": 0.8555844711418488,
      "step": 1500,
      "train/gate_loss": 91.76104736328125,
      "train/gate_loss_post": 18.691068649291992,
      "train/gate_loss_pre": 5.437891006469727,
      "train/lm_loss": 1.2393277883529663
    },
    {
      "epoch": 0.8555844711418488,
      "step": 1500,
      "train/gate_loss": 92.29264831542969,
      "train/gate_loss_post": 18.791603088378906,
      "train/gate_loss_pre": 5.470943927764893,
      "train/lm_loss": 1.4209933280944824
    },
    {
      "epoch": 0.8555844711418488,
      "step": 1500,
      "train/gate_loss": 92.42649841308594,
      "train/gate_loss_post": 18.86485481262207,
      "train/gate_loss_pre": 5.4696784019470215,
      "train/lm_loss": 1.0990954637527466
    },
    {
      "epoch": 0.8555844711418488,
      "step": 1500,
      "train/gate_loss": 91.9139633178711,
      "train/gate_loss_post": 18.814971923828125,
      "train/gate_loss_pre": 5.428401947021484,
      "train/lm_loss": 1.177061915397644
    },
    {
      "epoch": 0.8555844711418488,
      "step": 1500,
      "train/gate_loss": 91.77549743652344,
      "train/gate_loss_post": 18.873884201049805,
      "train/gate_loss_pre": 5.402772903442383,
      "train/lm_loss": 1.2569979429244995
    },
    {
      "epoch": 0.8555844711418488,
      "step": 1500,
      "train/gate_loss": 92.66094207763672,
      "train/gate_loss_post": 18.869861602783203,
      "train/gate_loss_pre": 5.492121696472168,
      "train/lm_loss": 1.2112234830856323
    },
    {
      "epoch": 0.8555844711418488,
      "step": 1500,
      "train/gate_loss": 92.34455108642578,
      "train/gate_loss_post": 18.811153411865234,
      "train/gate_loss_pre": 5.472224235534668,
      "train/lm_loss": 1.2148761749267578
    },
    {
      "epoch": 0.8555844711418488,
      "step": 1500,
      "train/gate_loss": 91.63259887695312,
      "train/gate_loss_post": 18.762897491455078,
      "train/gate_loss_pre": 5.410680770874023,
      "train/lm_loss": 1.1286591291427612
    },
    {
      "epoch": 0.9126234358846387,
      "grad_norm": 0.3786868155002594,
      "learning_rate": 7.558926202041498e-06,
      "loss": 1494.9478,
      "step": 1600
    },
    {
      "epoch": 0.9126234358846387,
      "step": 1600,
      "train/gate_loss": 93.09719848632812,
      "train/gate_loss_post": 18.8555908203125,
      "train/gate_loss_pre": 5.538601875305176,
      "train/lm_loss": 1.2308677434921265
    },
    {
      "epoch": 0.9126234358846387,
      "step": 1600,
      "train/gate_loss": 91.84664916992188,
      "train/gate_loss_post": 18.703044891357422,
      "train/gate_loss_pre": 5.444055557250977,
      "train/lm_loss": 1.2195087671279907
    },
    {
      "epoch": 0.9126234358846387,
      "step": 1600,
      "train/gate_loss": 92.32821655273438,
      "train/gate_loss_post": 18.73643684387207,
      "train/gate_loss_pre": 5.485533714294434,
      "train/lm_loss": 1.0890742540359497
    },
    {
      "epoch": 0.9126234358846387,
      "step": 1600,
      "train/gate_loss": 92.24921417236328,
      "train/gate_loss_post": 18.75528907775879,
      "train/gate_loss_pre": 5.47386360168457,
      "train/lm_loss": 1.1706777811050415
    },
    {
      "epoch": 0.9126234358846387,
      "step": 1600,
      "train/gate_loss": 92.0296630859375,
      "train/gate_loss_post": 18.727638244628906,
      "train/gate_loss_pre": 5.457438945770264,
      "train/lm_loss": 1.1536304950714111
    },
    {
      "epoch": 0.9126234358846387,
      "step": 1600,
      "train/gate_loss": 93.0200424194336,
      "train/gate_loss_post": 18.811508178710938,
      "train/gate_loss_pre": 5.539702415466309,
      "train/lm_loss": 1.2235355377197266
    },
    {
      "epoch": 0.9126234358846387,
      "step": 1600,
      "train/gate_loss": 92.8497543334961,
      "train/gate_loss_post": 18.839149475097656,
      "train/gate_loss_pre": 5.51714563369751,
      "train/lm_loss": 0.9637958407402039
    },
    {
      "epoch": 0.9126234358846387,
      "step": 1600,
      "train/gate_loss": 92.69822692871094,
      "train/gate_loss_post": 18.930456161499023,
      "train/gate_loss_pre": 5.483731746673584,
      "train/lm_loss": 1.239247441291809
    },
    {
      "epoch": 0.9126234358846387,
      "step": 1600,
      "train/gate_loss": 91.36676025390625,
      "train/gate_loss_post": 18.66352081298828,
      "train/gate_loss_pre": 5.403972148895264,
      "train/lm_loss": 1.1596251726150513
    },
    {
      "epoch": 0.9126234358846387,
      "step": 1600,
      "train/gate_loss": 91.75408172607422,
      "train/gate_loss_post": 18.76618003845215,
      "train/gate_loss_pre": 5.4221720695495605,
      "train/lm_loss": 1.0045042037963867
    },
    {
      "epoch": 0.9126234358846387,
      "step": 1600,
      "train/gate_loss": 92.2838134765625,
      "train/gate_loss_post": 18.848588943481445,
      "train/gate_loss_pre": 5.458662986755371,
      "train/lm_loss": 1.0710248947143555
    },
    {
      "epoch": 0.9126234358846387,
      "step": 1600,
      "train/gate_loss": 92.090087890625,
      "train/gate_loss_post": 18.92746925354004,
      "train/gate_loss_pre": 5.423515319824219,
      "train/lm_loss": 1.1405075788497925
    },
    {
      "epoch": 0.9126234358846387,
      "step": 1600,
      "train/gate_loss": 93.31098175048828,
      "train/gate_loss_post": 19.024513244628906,
      "train/gate_loss_pre": 5.526195526123047,
      "train/lm_loss": 1.2716869115829468
    },
    {
      "epoch": 0.9126234358846387,
      "step": 1600,
      "train/gate_loss": 91.65392303466797,
      "train/gate_loss_post": 18.826889038085938,
      "train/gate_loss_pre": 5.400014400482178,
      "train/lm_loss": 1.159775972366333
    },
    {
      "epoch": 0.9126234358846387,
      "step": 1600,
      "train/gate_loss": 92.92144012451172,
      "train/gate_loss_post": 18.883975982666016,
      "train/gate_loss_pre": 5.5153489112854,
      "train/lm_loss": 1.246373176574707
    },
    {
      "epoch": 0.9126234358846387,
      "step": 1600,
      "train/gate_loss": 93.03413391113281,
      "train/gate_loss_post": 18.935583114624023,
      "train/gate_loss_pre": 5.516296863555908,
      "train/lm_loss": 1.2341949939727783
    },
    {
      "epoch": 0.9696624006274286,
      "grad_norm": 0.3682297170162201,
      "learning_rate": 1.4614519807320867e-06,
      "loss": 1496.038,
      "step": 1700
    },
    {
      "epoch": 0.9696624006274286,
      "step": 1700,
      "train/gate_loss": 93.33412170410156,
      "train/gate_loss_post": 18.969282150268555,
      "train/gate_loss_pre": 5.539555549621582,
      "train/lm_loss": 1.1835616827011108
    },
    {
      "epoch": 0.9696624006274286,
      "step": 1700,
      "train/gate_loss": 92.48448181152344,
      "train/gate_loss_post": 18.85172462463379,
      "train/gate_loss_pre": 5.478102684020996,
      "train/lm_loss": 1.2427647113800049
    },
    {
      "epoch": 0.9696624006274286,
      "step": 1700,
      "train/gate_loss": 92.74798583984375,
      "train/gate_loss_post": 18.730695724487305,
      "train/gate_loss_pre": 5.528658866882324,
      "train/lm_loss": 1.07248854637146
    },
    {
      "epoch": 0.9696624006274286,
      "step": 1700,
      "train/gate_loss": 92.50208282470703,
      "train/gate_loss_post": 18.805002212524414,
      "train/gate_loss_pre": 5.489207744598389,
      "train/lm_loss": 1.2865337133407593
    },
    {
      "epoch": 0.9696624006274286,
      "step": 1700,
      "train/gate_loss": 93.68376159667969,
      "train/gate_loss_post": 18.91808319091797,
      "train/gate_loss_pre": 5.584759712219238,
      "train/lm_loss": 1.2713364362716675
    },
    {
      "epoch": 0.9696624006274286,
      "step": 1700,
      "train/gate_loss": 91.96440124511719,
      "train/gate_loss_post": 18.73316764831543,
      "train/gate_loss_pre": 5.4498066902160645,
      "train/lm_loss": 1.1724028587341309
    },
    {
      "epoch": 0.9696624006274286,
      "step": 1700,
      "train/gate_loss": 92.47709655761719,
      "train/gate_loss_post": 18.68523597717285,
      "train/gate_loss_pre": 5.510663032531738,
      "train/lm_loss": 1.265328049659729
    },
    {
      "epoch": 0.9696624006274286,
      "step": 1700,
      "train/gate_loss": 92.1575927734375,
      "train/gate_loss_post": 18.735132217407227,
      "train/gate_loss_pre": 5.468732833862305,
      "train/lm_loss": 1.1770859956741333
    },
    {
      "epoch": 0.9696624006274286,
      "step": 1700,
      "train/gate_loss": 92.43032836914062,
      "train/gate_loss_post": 18.866430282592773,
      "train/gate_loss_pre": 5.469747066497803,
      "train/lm_loss": 1.315562129020691
    },
    {
      "epoch": 0.9696624006274286,
      "step": 1700,
      "train/gate_loss": 93.00078582763672,
      "train/gate_loss_post": 18.793319702148438,
      "train/gate_loss_pre": 5.541414737701416,
      "train/lm_loss": 1.1981091499328613
    },
    {
      "epoch": 0.9696624006274286,
      "step": 1700,
      "train/gate_loss": 92.93136596679688,
      "train/gate_loss_post": 18.881277084350586,
      "train/gate_loss_pre": 5.516880989074707,
      "train/lm_loss": 1.288134217262268
    },
    {
      "epoch": 0.9696624006274286,
      "step": 1700,
      "train/gate_loss": 92.70685577392578,
      "train/gate_loss_post": 18.81130599975586,
      "train/gate_loss_pre": 5.508424282073975,
      "train/lm_loss": 1.0815541744232178
    },
    {
      "epoch": 0.9696624006274286,
      "step": 1700,
      "train/gate_loss": 93.16693115234375,
      "train/gate_loss_post": 18.945663452148438,
      "train/gate_loss_pre": 5.527560234069824,
      "train/lm_loss": 1.1287394762039185
    },
    {
      "epoch": 0.9696624006274286,
      "step": 1700,
      "train/gate_loss": 91.67718505859375,
      "train/gate_loss_post": 18.72039794921875,
      "train/gate_loss_pre": 5.423639297485352,
      "train/lm_loss": 1.2673687934875488
    },
    {
      "epoch": 0.9696624006274286,
      "step": 1700,
      "train/gate_loss": 92.59724426269531,
      "train/gate_loss_post": 18.79745864868164,
      "train/gate_loss_pre": 5.500233173370361,
      "train/lm_loss": 1.298618197441101
    },
    {
      "epoch": 0.9696624006274286,
      "step": 1700,
      "train/gate_loss": 91.45982360839844,
      "train/gate_loss_post": 18.7683162689209,
      "train/gate_loss_pre": 5.392319679260254,
      "train/lm_loss": 1.109393835067749
    }
  ],
  "logging_steps": 100,
  "max_steps": 1754,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.589602929345663e+18,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
