{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 877,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 44.8174934387207,
      "train/gate_loss_post": 18.770755767822266,
      "train/gate_loss_pre": 3.2558422088623047,
      "train/lm_loss": 1.9238914251327515
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 43.094749450683594,
      "train/gate_loss_post": 18.760913848876953,
      "train/gate_loss_pre": 3.041729211807251,
      "train/lm_loss": 1.7715445756912231
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 44.165374755859375,
      "train/gate_loss_post": 18.728395462036133,
      "train/gate_loss_pre": 3.1796226501464844,
      "train/lm_loss": 1.8152594566345215
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 44.11096954345703,
      "train/gate_loss_post": 18.784685134887695,
      "train/gate_loss_pre": 3.165785312652588,
      "train/lm_loss": 1.8415030241012573
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 44.4552116394043,
      "train/gate_loss_post": 18.66033363342285,
      "train/gate_loss_pre": 3.2243597507476807,
      "train/lm_loss": 1.9438999891281128
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 43.949195861816406,
      "train/gate_loss_post": 18.729082107543945,
      "train/gate_loss_pre": 3.1525139808654785,
      "train/lm_loss": 1.94791841506958
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 44.17949676513672,
      "train/gate_loss_post": 18.812217712402344,
      "train/gate_loss_pre": 3.170909881591797,
      "train/lm_loss": 1.9688869714736938
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 44.047523498535156,
      "train/gate_loss_post": 18.707042694091797,
      "train/gate_loss_pre": 3.16756010055542,
      "train/lm_loss": 1.9027661085128784
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 44.52912139892578,
      "train/gate_loss_post": 18.816207885742188,
      "train/gate_loss_pre": 3.2141144275665283,
      "train/lm_loss": 1.8558770418167114
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 43.99793243408203,
      "train/gate_loss_post": 18.69245719909668,
      "train/gate_loss_pre": 3.16318416595459,
      "train/lm_loss": 1.9000611305236816
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 44.431827545166016,
      "train/gate_loss_post": 18.67041015625,
      "train/gate_loss_pre": 3.220177173614502,
      "train/lm_loss": 1.8203133344650269
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 44.51390838623047,
      "train/gate_loss_post": 18.839189529418945,
      "train/gate_loss_pre": 3.2093400955200195,
      "train/lm_loss": 2.145446300506592
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 44.092002868652344,
      "train/gate_loss_post": 18.661056518554688,
      "train/gate_loss_pre": 3.178868293762207,
      "train/lm_loss": 1.973699927330017
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 43.985477447509766,
      "train/gate_loss_post": 18.70183753967285,
      "train/gate_loss_pre": 3.1604549884796143,
      "train/lm_loss": 1.9876145124435425
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 43.91424560546875,
      "train/gate_loss_post": 18.689207077026367,
      "train/gate_loss_pre": 3.1531295776367188,
      "train/lm_loss": 1.9863194227218628
    },
    {
      "epoch": 0,
      "step": 0,
      "train/gate_loss": 43.98331832885742,
      "train/gate_loss_post": 18.709402084350586,
      "train/gate_loss_pre": 3.1592395305633545,
      "train/lm_loss": 1.9845691919326782
    },
    {
      "epoch": 0.11407386282617996,
      "grad_norm": 12.704141616821289,
      "learning_rate": 0.0002906658295505804,
      "loss": 735.9835,
      "step": 100
    },
    {
      "epoch": 0.11407386282617996,
      "step": 100,
      "train/gate_loss": 44.887229919433594,
      "train/gate_loss_post": 18.55661964416504,
      "train/gate_loss_pre": 3.2913260459899902,
      "train/lm_loss": 1.5363508462905884
    },
    {
      "epoch": 0.11407386282617996,
      "step": 100,
      "train/gate_loss": 44.420982360839844,
      "train/gate_loss_post": 18.655168533325195,
      "train/gate_loss_pre": 3.220726490020752,
      "train/lm_loss": 1.4383832216262817
    },
    {
      "epoch": 0.11407386282617996,
      "step": 100,
      "train/gate_loss": 44.99466323852539,
      "train/gate_loss_post": 18.704627990722656,
      "train/gate_loss_pre": 3.286254405975342,
      "train/lm_loss": 1.5171209573745728
    },
    {
      "epoch": 0.11407386282617996,
      "step": 100,
      "train/gate_loss": 44.635101318359375,
      "train/gate_loss_post": 18.619131088256836,
      "train/gate_loss_pre": 3.2519965171813965,
      "train/lm_loss": 1.5378375053405762
    },
    {
      "epoch": 0.11407386282617996,
      "step": 100,
      "train/gate_loss": 44.99059295654297,
      "train/gate_loss_post": 18.694530487060547,
      "train/gate_loss_pre": 3.2870078086853027,
      "train/lm_loss": 1.4435452222824097
    },
    {
      "epoch": 0.11407386282617996,
      "step": 100,
      "train/gate_loss": 44.97224426269531,
      "train/gate_loss_post": 18.697505950927734,
      "train/gate_loss_pre": 3.2843422889709473,
      "train/lm_loss": 1.4588180780410767
    },
    {
      "epoch": 0.11407386282617996,
      "step": 100,
      "train/gate_loss": 44.65637969970703,
      "train/gate_loss_post": 18.66232681274414,
      "train/gate_loss_pre": 3.2492568492889404,
      "train/lm_loss": 1.5863810777664185
    },
    {
      "epoch": 0.11407386282617996,
      "step": 100,
      "train/gate_loss": 44.26902770996094,
      "train/gate_loss_post": 18.632661819458008,
      "train/gate_loss_pre": 3.2045459747314453,
      "train/lm_loss": 1.7578469514846802
    },
    {
      "epoch": 0.11407386282617996,
      "step": 100,
      "train/gate_loss": 44.29248809814453,
      "train/gate_loss_post": 18.611499786376953,
      "train/gate_loss_pre": 3.2101237773895264,
      "train/lm_loss": 1.489304542541504
    },
    {
      "epoch": 0.11407386282617996,
      "step": 100,
      "train/gate_loss": 45.07562255859375,
      "train/gate_loss_post": 18.721031188964844,
      "train/gate_loss_pre": 3.294323682785034,
      "train/lm_loss": 1.489766001701355
    },
    {
      "epoch": 0.11407386282617996,
      "step": 100,
      "train/gate_loss": 44.73255920410156,
      "train/gate_loss_post": 18.523181915283203,
      "train/gate_loss_pre": 3.276172399520874,
      "train/lm_loss": 1.7027745246887207
    },
    {
      "epoch": 0.11407386282617996,
      "step": 100,
      "train/gate_loss": 44.793025970458984,
      "train/gate_loss_post": 18.72918128967285,
      "train/gate_loss_pre": 3.2579805850982666,
      "train/lm_loss": 1.5077999830245972
    },
    {
      "epoch": 0.11407386282617996,
      "step": 100,
      "train/gate_loss": 44.47719192504883,
      "train/gate_loss_post": 18.5681095123291,
      "train/gate_loss_pre": 3.238635301589966,
      "train/lm_loss": 1.564516305923462
    },
    {
      "epoch": 0.11407386282617996,
      "step": 100,
      "train/gate_loss": 44.9315185546875,
      "train/gate_loss_post": 18.68895721435547,
      "train/gate_loss_pre": 3.280319929122925,
      "train/lm_loss": 1.599453330039978
    },
    {
      "epoch": 0.11407386282617996,
      "step": 100,
      "train/gate_loss": 44.967857360839844,
      "train/gate_loss_post": 18.682294845581055,
      "train/gate_loss_pre": 3.2856953144073486,
      "train/lm_loss": 1.600203275680542
    },
    {
      "epoch": 0.11407386282617996,
      "step": 100,
      "train/gate_loss": 44.75135040283203,
      "train/gate_loss_post": 18.75800895690918,
      "train/gate_loss_pre": 3.2491679191589355,
      "train/lm_loss": 1.4375728368759155
    },
    {
      "epoch": 0.2281477256523599,
      "grad_norm": 2.632327079772949,
      "learning_rate": 0.0002638250080420392,
      "loss": 761.5703,
      "step": 200
    },
    {
      "epoch": 0.2281477256523599,
      "step": 200,
      "train/gate_loss": 46.88676071166992,
      "train/gate_loss_post": 18.59478759765625,
      "train/gate_loss_pre": 3.536496639251709,
      "train/lm_loss": 1.3442312479019165
    },
    {
      "epoch": 0.2281477256523599,
      "step": 200,
      "train/gate_loss": 47.21967315673828,
      "train/gate_loss_post": 18.646318435668945,
      "train/gate_loss_pre": 3.571669101715088,
      "train/lm_loss": 1.3113025426864624
    },
    {
      "epoch": 0.2281477256523599,
      "step": 200,
      "train/gate_loss": 47.12638854980469,
      "train/gate_loss_post": 18.743227005004883,
      "train/gate_loss_pre": 3.5478954315185547,
      "train/lm_loss": 1.2126163244247437
    },
    {
      "epoch": 0.2281477256523599,
      "step": 200,
      "train/gate_loss": 46.88633728027344,
      "train/gate_loss_post": 18.531055450439453,
      "train/gate_loss_pre": 3.544410467147827,
      "train/lm_loss": 1.1677463054656982
    },
    {
      "epoch": 0.2281477256523599,
      "step": 200,
      "train/gate_loss": 47.39036560058594,
      "train/gate_loss_post": 18.681106567382812,
      "train/gate_loss_pre": 3.5886573791503906,
      "train/lm_loss": 1.3451844453811646
    },
    {
      "epoch": 0.2281477256523599,
      "step": 200,
      "train/gate_loss": 46.253936767578125,
      "train/gate_loss_post": 18.70717430114746,
      "train/gate_loss_pre": 3.443345308303833,
      "train/lm_loss": 1.3048732280731201
    },
    {
      "epoch": 0.2281477256523599,
      "step": 200,
      "train/gate_loss": 47.26185607910156,
      "train/gate_loss_post": 18.790443420410156,
      "train/gate_loss_pre": 3.5589263439178467,
      "train/lm_loss": 1.125158667564392
    },
    {
      "epoch": 0.2281477256523599,
      "step": 200,
      "train/gate_loss": 47.464927673339844,
      "train/gate_loss_post": 18.83094024658203,
      "train/gate_loss_pre": 3.5792481899261475,
      "train/lm_loss": 1.3883583545684814
    },
    {
      "epoch": 0.2281477256523599,
      "step": 200,
      "train/gate_loss": 47.2506217956543,
      "train/gate_loss_post": 18.75444793701172,
      "train/gate_loss_pre": 3.5620217323303223,
      "train/lm_loss": 1.229153037071228
    },
    {
      "epoch": 0.2281477256523599,
      "step": 200,
      "train/gate_loss": 47.30400848388672,
      "train/gate_loss_post": 18.789621353149414,
      "train/gate_loss_pre": 3.564298391342163,
      "train/lm_loss": 1.401034951210022
    },
    {
      "epoch": 0.2281477256523599,
      "step": 200,
      "train/gate_loss": 46.56233215332031,
      "train/gate_loss_post": 18.560710906982422,
      "train/gate_loss_pre": 3.5002028942108154,
      "train/lm_loss": 1.4079177379608154
    },
    {
      "epoch": 0.2281477256523599,
      "step": 200,
      "train/gate_loss": 47.270057678222656,
      "train/gate_loss_post": 18.637435913085938,
      "train/gate_loss_pre": 3.57907772064209,
      "train/lm_loss": 1.254164695739746
    },
    {
      "epoch": 0.2281477256523599,
      "step": 200,
      "train/gate_loss": 47.07875061035156,
      "train/gate_loss_post": 18.61863136291504,
      "train/gate_loss_pre": 3.5575146675109863,
      "train/lm_loss": 1.243033766746521
    },
    {
      "epoch": 0.2281477256523599,
      "step": 200,
      "train/gate_loss": 47.19063186645508,
      "train/gate_loss_post": 18.69036865234375,
      "train/gate_loss_pre": 3.562532901763916,
      "train/lm_loss": 1.505006194114685
    },
    {
      "epoch": 0.2281477256523599,
      "step": 200,
      "train/gate_loss": 47.08563232421875,
      "train/gate_loss_post": 18.65705680847168,
      "train/gate_loss_pre": 3.5535717010498047,
      "train/lm_loss": 1.269223690032959
    },
    {
      "epoch": 0.2281477256523599,
      "step": 200,
      "train/gate_loss": 46.46858215332031,
      "train/gate_loss_post": 18.86581802368164,
      "train/gate_loss_pre": 3.450345277786255,
      "train/lm_loss": 1.4352761507034302
    },
    {
      "epoch": 0.34222158847853984,
      "grad_norm": 2.1083719730377197,
      "learning_rate": 0.00022234779203132986,
      "loss": 785.6705,
      "step": 300
    },
    {
      "epoch": 0.34222158847853984,
      "step": 300,
      "train/gate_loss": 48.46027755737305,
      "train/gate_loss_post": 18.662843704223633,
      "train/gate_loss_pre": 3.7246792316436768,
      "train/lm_loss": 1.0880324840545654
    },
    {
      "epoch": 0.34222158847853984,
      "step": 300,
      "train/gate_loss": 47.870452880859375,
      "train/gate_loss_post": 18.54367446899414,
      "train/gate_loss_pre": 3.6658473014831543,
      "train/lm_loss": 1.2343450784683228
    },
    {
      "epoch": 0.34222158847853984,
      "step": 300,
      "train/gate_loss": 48.416744232177734,
      "train/gate_loss_post": 18.510351181030273,
      "train/gate_loss_pre": 3.7382991313934326,
      "train/lm_loss": 1.2992925643920898
    },
    {
      "epoch": 0.34222158847853984,
      "step": 300,
      "train/gate_loss": 48.02918243408203,
      "train/gate_loss_post": 18.521987915039062,
      "train/gate_loss_pre": 3.688399314880371,
      "train/lm_loss": 1.2617161273956299
    },
    {
      "epoch": 0.34222158847853984,
      "step": 300,
      "train/gate_loss": 48.08757781982422,
      "train/gate_loss_post": 18.583614349365234,
      "train/gate_loss_pre": 3.687995672225952,
      "train/lm_loss": 1.2845089435577393
    },
    {
      "epoch": 0.34222158847853984,
      "step": 300,
      "train/gate_loss": 48.438865661621094,
      "train/gate_loss_post": 18.641429901123047,
      "train/gate_loss_pre": 3.724679470062256,
      "train/lm_loss": 1.1715753078460693
    },
    {
      "epoch": 0.34222158847853984,
      "step": 300,
      "train/gate_loss": 48.26586151123047,
      "train/gate_loss_post": 18.669645309448242,
      "train/gate_loss_pre": 3.699526786804199,
      "train/lm_loss": 1.2224880456924438
    },
    {
      "epoch": 0.34222158847853984,
      "step": 300,
      "train/gate_loss": 48.519935607910156,
      "train/gate_loss_post": 18.68074607849121,
      "train/gate_loss_pre": 3.7298989295959473,
      "train/lm_loss": 1.186454176902771
    },
    {
      "epoch": 0.34222158847853984,
      "step": 300,
      "train/gate_loss": 48.36236572265625,
      "train/gate_loss_post": 18.579835891723633,
      "train/gate_loss_pre": 3.7228164672851562,
      "train/lm_loss": 1.1623800992965698
    },
    {
      "epoch": 0.34222158847853984,
      "step": 300,
      "train/gate_loss": 48.55762481689453,
      "train/gate_loss_post": 18.653921127319336,
      "train/gate_loss_pre": 3.7379627227783203,
      "train/lm_loss": 1.1757044792175293
    },
    {
      "epoch": 0.34222158847853984,
      "step": 300,
      "train/gate_loss": 48.34051513671875,
      "train/gate_loss_post": 18.60113525390625,
      "train/gate_loss_pre": 3.7174227237701416,
      "train/lm_loss": 1.3933297395706177
    },
    {
      "epoch": 0.34222158847853984,
      "step": 300,
      "train/gate_loss": 48.26258087158203,
      "train/gate_loss_post": 18.5885009765625,
      "train/gate_loss_pre": 3.7092599868774414,
      "train/lm_loss": 1.2402619123458862
    },
    {
      "epoch": 0.34222158847853984,
      "step": 300,
      "train/gate_loss": 48.043617248535156,
      "train/gate_loss_post": 18.51647186279297,
      "train/gate_loss_pre": 3.6908931732177734,
      "train/lm_loss": 1.2707633972167969
    },
    {
      "epoch": 0.34222158847853984,
      "step": 300,
      "train/gate_loss": 48.47440719604492,
      "train/gate_loss_post": 18.521623611450195,
      "train/gate_loss_pre": 3.744097948074341,
      "train/lm_loss": 1.3586382865905762
    },
    {
      "epoch": 0.34222158847853984,
      "step": 300,
      "train/gate_loss": 48.49672317504883,
      "train/gate_loss_post": 18.717466354370117,
      "train/gate_loss_pre": 3.722407102584839,
      "train/lm_loss": 1.1648849248886108
    },
    {
      "epoch": 0.34222158847853984,
      "step": 300,
      "train/gate_loss": 48.256317138671875,
      "train/gate_loss_post": 18.486862182617188,
      "train/gate_loss_pre": 3.721181631088257,
      "train/lm_loss": 1.2514286041259766
    },
    {
      "epoch": 0.4562954513047198,
      "grad_norm": 2.0567524433135986,
      "learning_rate": 0.00017168562717725908,
      "loss": 808.0456,
      "step": 400
    },
    {
      "epoch": 0.4562954513047198,
      "step": 400,
      "train/gate_loss": 49.26453399658203,
      "train/gate_loss_post": 18.50074577331543,
      "train/gate_loss_pre": 3.845473527908325,
      "train/lm_loss": 1.1573840379714966
    },
    {
      "epoch": 0.4562954513047198,
      "step": 400,
      "train/gate_loss": 49.183074951171875,
      "train/gate_loss_post": 18.57016372680664,
      "train/gate_loss_pre": 3.8266141414642334,
      "train/lm_loss": 1.1822720766067505
    },
    {
      "epoch": 0.4562954513047198,
      "step": 400,
      "train/gate_loss": 48.69556427001953,
      "train/gate_loss_post": 18.543840408325195,
      "train/gate_loss_pre": 3.768965482711792,
      "train/lm_loss": 1.160750389099121
    },
    {
      "epoch": 0.4562954513047198,
      "step": 400,
      "train/gate_loss": 49.570716857910156,
      "train/gate_loss_post": 18.58061408996582,
      "train/gate_loss_pre": 3.873762607574463,
      "train/lm_loss": 1.1551114320755005
    },
    {
      "epoch": 0.4562954513047198,
      "step": 400,
      "train/gate_loss": 49.35165023803711,
      "train/gate_loss_post": 18.5105037689209,
      "train/gate_loss_pre": 3.8551433086395264,
      "train/lm_loss": 1.14093017578125
    },
    {
      "epoch": 0.4562954513047198,
      "step": 400,
      "train/gate_loss": 49.179412841796875,
      "train/gate_loss_post": 18.57817268371582,
      "train/gate_loss_pre": 3.8251547813415527,
      "train/lm_loss": 1.3644044399261475
    },
    {
      "epoch": 0.4562954513047198,
      "step": 400,
      "train/gate_loss": 49.329612731933594,
      "train/gate_loss_post": 18.568126678466797,
      "train/gate_loss_pre": 3.8451859951019287,
      "train/lm_loss": 1.180751085281372
    },
    {
      "epoch": 0.4562954513047198,
      "step": 400,
      "train/gate_loss": 49.54907989501953,
      "train/gate_loss_post": 18.64568328857422,
      "train/gate_loss_pre": 3.862924814224243,
      "train/lm_loss": 1.1508090496063232
    },
    {
      "epoch": 0.4562954513047198,
      "step": 400,
      "train/gate_loss": 49.20545196533203,
      "train/gate_loss_post": 18.65273666381836,
      "train/gate_loss_pre": 3.819089412689209,
      "train/lm_loss": 1.437994122505188
    },
    {
      "epoch": 0.4562954513047198,
      "step": 400,
      "train/gate_loss": 49.261207580566406,
      "train/gate_loss_post": 18.673755645751953,
      "train/gate_loss_pre": 3.8234312534332275,
      "train/lm_loss": 1.1336174011230469
    },
    {
      "epoch": 0.4562954513047198,
      "step": 400,
      "train/gate_loss": 49.32453155517578,
      "train/gate_loss_post": 18.483932495117188,
      "train/gate_loss_pre": 3.8550751209259033,
      "train/lm_loss": 1.0477334260940552
    },
    {
      "epoch": 0.4562954513047198,
      "step": 400,
      "train/gate_loss": 49.181236267089844,
      "train/gate_loss_post": 18.502498626708984,
      "train/gate_loss_pre": 3.8348422050476074,
      "train/lm_loss": 1.1164253950119019
    },
    {
      "epoch": 0.4562954513047198,
      "step": 400,
      "train/gate_loss": 48.86201858520508,
      "train/gate_loss_post": 18.500560760498047,
      "train/gate_loss_pre": 3.795182228088379,
      "train/lm_loss": 1.1770014762878418
    },
    {
      "epoch": 0.4562954513047198,
      "step": 400,
      "train/gate_loss": 49.62940979003906,
      "train/gate_loss_post": 18.599613189697266,
      "train/gate_loss_pre": 3.8787243366241455,
      "train/lm_loss": 1.1176512241363525
    },
    {
      "epoch": 0.4562954513047198,
      "step": 400,
      "train/gate_loss": 48.82158660888672,
      "train/gate_loss_post": 18.439912796020508,
      "train/gate_loss_pre": 3.7977094650268555,
      "train/lm_loss": 1.1185606718063354
    },
    {
      "epoch": 0.4562954513047198,
      "step": 400,
      "train/gate_loss": 49.187286376953125,
      "train/gate_loss_post": 18.55230712890625,
      "train/gate_loss_pre": 3.8293726444244385,
      "train/lm_loss": 1.1373196840286255
    },
    {
      "epoch": 0.5703693141308998,
      "grad_norm": 2.112626075744629,
      "learning_rate": 0.00011827035306211946,
      "loss": 820.6007,
      "step": 500
    },
    {
      "epoch": 0.5703693141308998,
      "step": 500,
      "train/gate_loss": 49.59528732299805,
      "train/gate_loss_post": 18.67919158935547,
      "train/gate_loss_pre": 3.8645119667053223,
      "train/lm_loss": 1.1698695421218872
    },
    {
      "epoch": 0.5703693141308998,
      "step": 500,
      "train/gate_loss": 49.95843505859375,
      "train/gate_loss_post": 18.671327590942383,
      "train/gate_loss_pre": 3.910888195037842,
      "train/lm_loss": 1.2629215717315674
    },
    {
      "epoch": 0.5703693141308998,
      "step": 500,
      "train/gate_loss": 49.862205505371094,
      "train/gate_loss_post": 18.607250213623047,
      "train/gate_loss_pre": 3.906869649887085,
      "train/lm_loss": 1.0242862701416016
    },
    {
      "epoch": 0.5703693141308998,
      "step": 500,
      "train/gate_loss": 50.155303955078125,
      "train/gate_loss_post": 18.570796966552734,
      "train/gate_loss_pre": 3.9480631351470947,
      "train/lm_loss": 1.2459437847137451
    },
    {
      "epoch": 0.5703693141308998,
      "step": 500,
      "train/gate_loss": 50.0118522644043,
      "train/gate_loss_post": 18.520370483398438,
      "train/gate_loss_pre": 3.9364352226257324,
      "train/lm_loss": 1.1107866764068604
    },
    {
      "epoch": 0.5703693141308998,
      "step": 500,
      "train/gate_loss": 49.558502197265625,
      "train/gate_loss_post": 18.674137115478516,
      "train/gate_loss_pre": 3.8605456352233887,
      "train/lm_loss": 1.1003363132476807
    },
    {
      "epoch": 0.5703693141308998,
      "step": 500,
      "train/gate_loss": 49.330543518066406,
      "train/gate_loss_post": 18.6496524810791,
      "train/gate_loss_pre": 3.835111618041992,
      "train/lm_loss": 1.0656044483184814
    },
    {
      "epoch": 0.5703693141308998,
      "step": 500,
      "train/gate_loss": 50.12611389160156,
      "train/gate_loss_post": 18.725778579711914,
      "train/gate_loss_pre": 3.925041913986206,
      "train/lm_loss": 1.010366439819336
    },
    {
      "epoch": 0.5703693141308998,
      "step": 500,
      "train/gate_loss": 49.954925537109375,
      "train/gate_loss_post": 18.534276962280273,
      "train/gate_loss_pre": 3.9275808334350586,
      "train/lm_loss": 1.1290760040283203
    },
    {
      "epoch": 0.5703693141308998,
      "step": 500,
      "train/gate_loss": 49.92304229736328,
      "train/gate_loss_post": 18.538593292236328,
      "train/gate_loss_pre": 3.9230563640594482,
      "train/lm_loss": 1.0581501722335815
    },
    {
      "epoch": 0.5703693141308998,
      "step": 500,
      "train/gate_loss": 49.39888000488281,
      "train/gate_loss_post": 18.566797256469727,
      "train/gate_loss_pre": 3.854010581970215,
      "train/lm_loss": 1.2473677396774292
    },
    {
      "epoch": 0.5703693141308998,
      "step": 500,
      "train/gate_loss": 49.848358154296875,
      "train/gate_loss_post": 18.6875,
      "train/gate_loss_pre": 3.8951072692871094,
      "train/lm_loss": 1.222471833229065
    },
    {
      "epoch": 0.5703693141308998,
      "step": 500,
      "train/gate_loss": 50.294342041015625,
      "train/gate_loss_post": 18.604122161865234,
      "train/gate_loss_pre": 3.9612772464752197,
      "train/lm_loss": 1.179238200187683
    },
    {
      "epoch": 0.5703693141308998,
      "step": 500,
      "train/gate_loss": 49.41742706298828,
      "train/gate_loss_post": 18.460994720458984,
      "train/gate_loss_pre": 3.869554281234741,
      "train/lm_loss": 1.121422529220581
    },
    {
      "epoch": 0.5703693141308998,
      "step": 500,
      "train/gate_loss": 50.662445068359375,
      "train/gate_loss_post": 18.659576416015625,
      "train/gate_loss_pre": 4.000358581542969,
      "train/lm_loss": 1.0402584075927734
    },
    {
      "epoch": 0.5703693141308998,
      "step": 500,
      "train/gate_loss": 49.799373626708984,
      "train/gate_loss_post": 18.533281326293945,
      "train/gate_loss_pre": 3.90826153755188,
      "train/lm_loss": 1.1659049987792969
    },
    {
      "epoch": 0.6844431769570797,
      "grad_norm": 2.2217817306518555,
      "learning_rate": 6.888333158274998e-05,
      "loss": 830.8346,
      "step": 600
    },
    {
      "epoch": 0.6844431769570797,
      "step": 600,
      "train/gate_loss": 49.70475387573242,
      "train/gate_loss_post": 18.54694938659668,
      "train/gate_loss_pre": 3.8947255611419678,
      "train/lm_loss": 1.1545062065124512
    },
    {
      "epoch": 0.6844431769570797,
      "step": 600,
      "train/gate_loss": 50.540870666503906,
      "train/gate_loss_post": 18.66832733154297,
      "train/gate_loss_pre": 3.984067678451538,
      "train/lm_loss": 1.0387424230575562
    },
    {
      "epoch": 0.6844431769570797,
      "step": 600,
      "train/gate_loss": 49.859920501708984,
      "train/gate_loss_post": 18.54198455810547,
      "train/gate_loss_pre": 3.9147419929504395,
      "train/lm_loss": 1.0407675504684448
    },
    {
      "epoch": 0.6844431769570797,
      "step": 600,
      "train/gate_loss": 50.308719635009766,
      "train/gate_loss_post": 18.55727767944336,
      "train/gate_loss_pre": 3.968930244445801,
      "train/lm_loss": 1.084975004196167
    },
    {
      "epoch": 0.6844431769570797,
      "step": 600,
      "train/gate_loss": 50.412391662597656,
      "train/gate_loss_post": 18.666975021362305,
      "train/gate_loss_pre": 3.968177318572998,
      "train/lm_loss": 1.1680824756622314
    },
    {
      "epoch": 0.6844431769570797,
      "step": 600,
      "train/gate_loss": 50.669273376464844,
      "train/gate_loss_post": 18.62415885925293,
      "train/gate_loss_pre": 4.005639553070068,
      "train/lm_loss": 1.1060999631881714
    },
    {
      "epoch": 0.6844431769570797,
      "step": 600,
      "train/gate_loss": 50.00733184814453,
      "train/gate_loss_post": 18.58903694152832,
      "train/gate_loss_pre": 3.9272871017456055,
      "train/lm_loss": 1.0209519863128662
    },
    {
      "epoch": 0.6844431769570797,
      "step": 600,
      "train/gate_loss": 50.04640197753906,
      "train/gate_loss_post": 18.64943504333496,
      "train/gate_loss_pre": 3.924621105194092,
      "train/lm_loss": 1.179365873336792
    },
    {
      "epoch": 0.6844431769570797,
      "step": 600,
      "train/gate_loss": 49.55845642089844,
      "train/gate_loss_post": 18.5705509185791,
      "train/gate_loss_pre": 3.873488426208496,
      "train/lm_loss": 1.1506890058517456
    },
    {
      "epoch": 0.6844431769570797,
      "step": 600,
      "train/gate_loss": 50.26744842529297,
      "train/gate_loss_post": 18.656299591064453,
      "train/gate_loss_pre": 3.9513938426971436,
      "train/lm_loss": 1.1228055953979492
    },
    {
      "epoch": 0.6844431769570797,
      "step": 600,
      "train/gate_loss": 50.60426330566406,
      "train/gate_loss_post": 18.72614097595215,
      "train/gate_loss_pre": 3.9847655296325684,
      "train/lm_loss": 1.1151446104049683
    },
    {
      "epoch": 0.6844431769570797,
      "step": 600,
      "train/gate_loss": 50.58262252807617,
      "train/gate_loss_post": 18.706077575683594,
      "train/gate_loss_pre": 3.9845681190490723,
      "train/lm_loss": 1.0223459005355835
    },
    {
      "epoch": 0.6844431769570797,
      "step": 600,
      "train/gate_loss": 50.94865036010742,
      "train/gate_loss_post": 18.713504791259766,
      "train/gate_loss_pre": 4.029393196105957,
      "train/lm_loss": 1.0962437391281128
    },
    {
      "epoch": 0.6844431769570797,
      "step": 600,
      "train/gate_loss": 50.62080383300781,
      "train/gate_loss_post": 18.496349334716797,
      "train/gate_loss_pre": 4.015556812286377,
      "train/lm_loss": 1.25844144821167
    },
    {
      "epoch": 0.6844431769570797,
      "step": 600,
      "train/gate_loss": 50.163970947265625,
      "train/gate_loss_post": 18.59477424621582,
      "train/gate_loss_pre": 3.9461493492126465,
      "train/lm_loss": 1.2171595096588135
    },
    {
      "epoch": 0.6844431769570797,
      "step": 600,
      "train/gate_loss": 50.55011749267578,
      "train/gate_loss_post": 18.719816207885742,
      "train/gate_loss_pre": 3.978787899017334,
      "train/lm_loss": 1.029507040977478
    },
    {
      "epoch": 0.7985170397832596,
      "grad_norm": 2.4855549335479736,
      "learning_rate": 2.9794515879082958e-05,
      "loss": 835.2127,
      "step": 700
    },
    {
      "epoch": 0.7985170397832596,
      "step": 700,
      "train/gate_loss": 50.463417053222656,
      "train/gate_loss_post": 18.5861759185791,
      "train/gate_loss_pre": 3.9846551418304443,
      "train/lm_loss": 1.1026848554611206
    },
    {
      "epoch": 0.7985170397832596,
      "step": 700,
      "train/gate_loss": 50.78330993652344,
      "train/gate_loss_post": 18.714197158813477,
      "train/gate_loss_pre": 4.008638858795166,
      "train/lm_loss": 1.0393683910369873
    },
    {
      "epoch": 0.7985170397832596,
      "step": 700,
      "train/gate_loss": 48.881221771240234,
      "train/gate_loss_post": 18.696624755859375,
      "train/gate_loss_pre": 3.7730746269226074,
      "train/lm_loss": 1.0287082195281982
    },
    {
      "epoch": 0.7985170397832596,
      "step": 700,
      "train/gate_loss": 49.86328887939453,
      "train/gate_loss_post": 18.537904739379883,
      "train/gate_loss_pre": 3.91567325592041,
      "train/lm_loss": 1.1654831171035767
    },
    {
      "epoch": 0.7985170397832596,
      "step": 700,
      "train/gate_loss": 50.70098876953125,
      "train/gate_loss_post": 18.760231018066406,
      "train/gate_loss_pre": 3.9925947189331055,
      "train/lm_loss": 1.1673332452774048
    },
    {
      "epoch": 0.7985170397832596,
      "step": 700,
      "train/gate_loss": 50.44015121459961,
      "train/gate_loss_post": 18.783435821533203,
      "train/gate_loss_pre": 3.957089424133301,
      "train/lm_loss": 1.3152002096176147
    },
    {
      "epoch": 0.7985170397832596,
      "step": 700,
      "train/gate_loss": 50.54263687133789,
      "train/gate_loss_post": 18.58765983581543,
      "train/gate_loss_pre": 3.9943721294403076,
      "train/lm_loss": 1.0996403694152832
    },
    {
      "epoch": 0.7985170397832596,
      "step": 700,
      "train/gate_loss": 50.609405517578125,
      "train/gate_loss_post": 18.692005157470703,
      "train/gate_loss_pre": 3.9896748065948486,
      "train/lm_loss": 1.0322153568267822
    },
    {
      "epoch": 0.7985170397832596,
      "step": 700,
      "train/gate_loss": 50.72599411010742,
      "train/gate_loss_post": 18.62155532836914,
      "train/gate_loss_pre": 4.013054847717285,
      "train/lm_loss": 1.2396684885025024
    },
    {
      "epoch": 0.7985170397832596,
      "step": 700,
      "train/gate_loss": 50.28599548339844,
      "train/gate_loss_post": 18.632083892822266,
      "train/gate_loss_pre": 3.9567387104034424,
      "train/lm_loss": 1.0824941396713257
    },
    {
      "epoch": 0.7985170397832596,
      "step": 700,
      "train/gate_loss": 50.49977493286133,
      "train/gate_loss_post": 18.73157501220703,
      "train/gate_loss_pre": 3.971024990081787,
      "train/lm_loss": 1.0117422342300415
    },
    {
      "epoch": 0.7985170397832596,
      "step": 700,
      "train/gate_loss": 50.31999969482422,
      "train/gate_loss_post": 18.68187713623047,
      "train/gate_loss_pre": 3.9547650814056396,
      "train/lm_loss": 1.1185612678527832
    },
    {
      "epoch": 0.7985170397832596,
      "step": 700,
      "train/gate_loss": 50.749393463134766,
      "train/gate_loss_post": 18.652446746826172,
      "train/gate_loss_pre": 4.012118339538574,
      "train/lm_loss": 1.174647331237793
    },
    {
      "epoch": 0.7985170397832596,
      "step": 700,
      "train/gate_loss": 50.472625732421875,
      "train/gate_loss_post": 18.722248077392578,
      "train/gate_loss_pre": 3.968797445297241,
      "train/lm_loss": 1.1356549263000488
    },
    {
      "epoch": 0.7985170397832596,
      "step": 700,
      "train/gate_loss": 50.94215393066406,
      "train/gate_loss_post": 18.74689292907715,
      "train/gate_loss_pre": 4.024407386779785,
      "train/lm_loss": 1.248869776725769
    },
    {
      "epoch": 0.7985170397832596,
      "step": 700,
      "train/gate_loss": 50.68238067626953,
      "train/gate_loss_post": 18.64875030517578,
      "train/gate_loss_pre": 4.004203796386719,
      "train/lm_loss": 1.043258547782898
    },
    {
      "epoch": 0.9125909026094396,
      "grad_norm": 2.8302621841430664,
      "learning_rate": 5.966445408166215e-06,
      "loss": 834.4303,
      "step": 800
    },
    {
      "epoch": 0.9125909026094396,
      "step": 800,
      "train/gate_loss": 50.3192138671875,
      "train/gate_loss_post": 18.63090705871582,
      "train/gate_loss_pre": 3.961038112640381,
      "train/lm_loss": 1.2339341640472412
    },
    {
      "epoch": 0.9125909026094396,
      "step": 800,
      "train/gate_loss": 50.56667709350586,
      "train/gate_loss_post": 18.532085418701172,
      "train/gate_loss_pre": 4.004323959350586,
      "train/lm_loss": 1.2658041715621948
    },
    {
      "epoch": 0.9125909026094396,
      "step": 800,
      "train/gate_loss": 50.54445266723633,
      "train/gate_loss_post": 18.665691375732422,
      "train/gate_loss_pre": 3.9848451614379883,
      "train/lm_loss": 1.2401797771453857
    },
    {
      "epoch": 0.9125909026094396,
      "step": 800,
      "train/gate_loss": 50.483863830566406,
      "train/gate_loss_post": 18.639747619628906,
      "train/gate_loss_pre": 3.9805142879486084,
      "train/lm_loss": 1.0232938528060913
    },
    {
      "epoch": 0.9125909026094396,
      "step": 800,
      "train/gate_loss": 50.88618469238281,
      "train/gate_loss_post": 18.636064529418945,
      "train/gate_loss_pre": 4.0312652587890625,
      "train/lm_loss": 0.9228115081787109
    },
    {
      "epoch": 0.9125909026094396,
      "step": 800,
      "train/gate_loss": 50.62662887573242,
      "train/gate_loss_post": 18.619060516357422,
      "train/gate_loss_pre": 4.000946044921875,
      "train/lm_loss": 1.1380952596664429
    },
    {
      "epoch": 0.9125909026094396,
      "step": 800,
      "train/gate_loss": 50.62430953979492,
      "train/gate_loss_post": 18.714763641357422,
      "train/gate_loss_pre": 3.9886932373046875,
      "train/lm_loss": 1.1310900449752808
    },
    {
      "epoch": 0.9125909026094396,
      "step": 800,
      "train/gate_loss": 50.253089904785156,
      "train/gate_loss_post": 18.63983726501465,
      "train/gate_loss_pre": 3.9516563415527344,
      "train/lm_loss": 1.2806272506713867
    },
    {
      "epoch": 0.9125909026094396,
      "step": 800,
      "train/gate_loss": 51.09040451049805,
      "train/gate_loss_post": 18.81207275390625,
      "train/gate_loss_pre": 4.034791469573975,
      "train/lm_loss": 1.124527096748352
    },
    {
      "epoch": 0.9125909026094396,
      "step": 800,
      "train/gate_loss": 50.73356246948242,
      "train/gate_loss_post": 18.707340240478516,
      "train/gate_loss_pre": 4.003277778625488,
      "train/lm_loss": 1.0584547519683838
    },
    {
      "epoch": 0.9125909026094396,
      "step": 800,
      "train/gate_loss": 50.58654022216797,
      "train/gate_loss_post": 18.61284065246582,
      "train/gate_loss_pre": 3.9967126846313477,
      "train/lm_loss": 1.063437581062317
    },
    {
      "epoch": 0.9125909026094396,
      "step": 800,
      "train/gate_loss": 50.48326110839844,
      "train/gate_loss_post": 18.691123962402344,
      "train/gate_loss_pre": 3.9740169048309326,
      "train/lm_loss": 1.161460518836975
    },
    {
      "epoch": 0.9125909026094396,
      "step": 800,
      "train/gate_loss": 50.07093048095703,
      "train/gate_loss_post": 18.697343826293945,
      "train/gate_loss_pre": 3.9216980934143066,
      "train/lm_loss": 1.0529216527938843
    },
    {
      "epoch": 0.9125909026094396,
      "step": 800,
      "train/gate_loss": 50.33656311035156,
      "train/gate_loss_post": 18.677242279052734,
      "train/gate_loss_pre": 3.9574151039123535,
      "train/lm_loss": 1.168670892715454
    },
    {
      "epoch": 0.9125909026094396,
      "step": 800,
      "train/gate_loss": 50.34105682373047,
      "train/gate_loss_post": 18.761777877807617,
      "train/gate_loss_pre": 3.9474101066589355,
      "train/lm_loss": 1.0968785285949707
    },
    {
      "epoch": 0.9125909026094396,
      "step": 800,
      "train/gate_loss": 51.122589111328125,
      "train/gate_loss_post": 18.69902229309082,
      "train/gate_loss_pre": 4.052946090698242,
      "train/lm_loss": 1.03926420211792
    }
  ],
  "logging_steps": 100,
  "max_steps": 877,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7.929301343273165e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
